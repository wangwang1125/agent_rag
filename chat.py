import os
import logging
import warnings
from openai import OpenAI
from llama_index.core import StorageContext,load_index_from_storage,Settings
from llama_index.embeddings.dashscope import (
    DashScopeEmbedding,
    DashScopeTextEmbeddingModels,
    DashScopeTextEmbeddingType,
)
from llama_index.postprocessor.dashscope_rerank import DashScopeRerank
from create_kb import *

# å¯¼å…¥å¤šæ™ºèƒ½ä½“ç›¸å…³æ¨¡å—
from dashscope import Assistants, Messages, Runs, Threads
import dashscope
import json
import ast
from tools import MedicalAnalysis, HealthAssessment

# ç¦ç”¨DashScopeå’Œç›¸å…³åº“çš„æ—¥å¿—è¾“å‡º
logging.getLogger('dashscope').setLevel(logging.ERROR)
logging.getLogger('openai').setLevel(logging.ERROR)
logging.getLogger('httpx').setLevel(logging.ERROR)
logging.getLogger('httpcore').setLevel(logging.ERROR)
logging.getLogger('requests').setLevel(logging.ERROR)
logging.getLogger('urllib3').setLevel(logging.ERROR)

# ç¦ç”¨æ‰€æœ‰è­¦å‘Š
warnings.filterwarnings('ignore')

# è®¾ç½®æ ¹æ—¥å¿—çº§åˆ«ä¸ºWARNINGï¼Œé¿å…è°ƒè¯•ä¿¡æ¯è¾“å‡º
logging.basicConfig(level=logging.WARNING)

# å…¨å±€è¿‡æ»¤DashScopeè°ƒè¯•è¾“å‡ºçš„è§£å†³æ–¹æ¡ˆ
import sys
import contextlib

class DashScopeOutputFilter:
    """å…¨å±€è¿‡æ»¤DashScope JSONè°ƒè¯•è¾“å‡ºçš„ç±»"""
    def __init__(self, original_stdout, original_stderr):
        self.original_stdout = original_stdout
        self.original_stderr = original_stderr
        self.suppress_next_newlines = False  # æ ‡è®°æ˜¯å¦éœ€è¦æŠ‘åˆ¶åç»­çš„æ¢è¡Œ
        
    def write(self, text):
        if text and isinstance(text, str):
            # è¿‡æ»¤DashScopeçš„JSONè°ƒè¯•è¾“å‡º
            text_lower = text.lower()
            if (text.strip().startswith('{') and 
                any(keyword in text for keyword in ['assistant_id', 'thread_id', 'run_id', 'object": "thread.run'])):
                self.suppress_next_newlines = True  # è®¾ç½®æ ‡è®°ï¼ŒæŠ‘åˆ¶åç»­æ¢è¡Œ
                return  # ä¸è¾“å‡º
            if any(keyword in text_lower for keyword in [
                'status_code', 'request_id', 'created_at', 'instructions'
            ]) and text.strip().startswith('{'):
                self.suppress_next_newlines = True  # è®¾ç½®æ ‡è®°ï¼ŒæŠ‘åˆ¶åç»­æ¢è¡Œ
                return  # ä¸è¾“å‡º
            
            # å¦‚æœå½“å‰åœ¨æŠ‘åˆ¶æ¨¡å¼ï¼Œä¸”æ–‡æœ¬åªåŒ…å«ç©ºç™½å­—ç¬¦ï¼ˆæ¢è¡Œç¬¦ã€ç©ºæ ¼ç­‰ï¼‰ï¼Œåˆ™ä¸è¾“å‡º
            if self.suppress_next_newlines:
                if text.strip() == '':  # åªåŒ…å«ç©ºç™½å­—ç¬¦
                    return  # ä¸è¾“å‡ºç©ºç™½è¡Œ
                else:
                    # é‡åˆ°éç©ºç™½å†…å®¹ï¼Œå–æ¶ˆæŠ‘åˆ¶æ¨¡å¼
                    self.suppress_next_newlines = False
                    
        # è¾“å‡ºåˆ°åŸå§‹stdout
        self.original_stdout.write(text)
        
    def flush(self):
        self.original_stdout.flush()

# åº”ç”¨å…¨å±€è¿‡æ»¤å™¨
original_stdout = sys.stdout
original_stderr = sys.stderr
sys.stdout = DashScopeOutputFilter(original_stdout, original_stderr)

# è®¾ç½®DashScope APIå¯†é’¥
dashscope.api_key = "sk-51d30a5436ca433b8ff81e624a23dcac"

# è¿›ä¸€æ­¥æ§åˆ¶DashScopeè¾“å‡º - è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['DASHSCOPE_DEBUG'] = 'false'
os.environ['DASHSCOPE_VERBOSE'] = 'false'
os.environ['OPENAI_LOG_LEVEL'] = 'error'

# å¦‚æœDashScopeæœ‰é…ç½®é€‰é¡¹ï¼Œè®¾ç½®ä¸ºé™é»˜æ¨¡å¼
try:
    dashscope.api_base = dashscope.api_base  # ä¿æŒé»˜è®¤å€¼
    # å°è¯•è®¾ç½®è°ƒè¯•æ¨¡å¼ä¸ºFalseï¼ˆå¦‚æœæ”¯æŒï¼‰
    if hasattr(dashscope, 'debug'):
        dashscope.debug = False
    if hasattr(dashscope, 'verbose'):
        dashscope.verbose = False
except:
    pass

DB_PATH = "VectorStore"
TMP_NAME = "tmp_abcd"
EMBED_MODEL = DashScopeEmbedding(
    model_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V2,
    text_type=DashScopeTextEmbeddingType.TEXT_TYPE_DOCUMENT,
    api_key="sk-51d30a5436ca433b8ff81e624a23dcac",
)
# è‹¥ä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹ï¼Œè¯·å–æ¶ˆä»¥ä¸‹æ³¨é‡Šï¼š
# from langchain_community.embeddings import ModelScopeEmbeddings
# from llama_index.embeddings.langchain import LangchainEmbedding
# embeddings = ModelScopeEmbeddings(model_id="modelscope/iic/nlp_gte_sentence-embedding_chinese-large")
# EMBED_MODEL = LangchainEmbedding(embeddings)

# è®¾ç½®åµŒå…¥æ¨¡å‹
Settings.embed_model = EMBED_MODEL

# ==================== å¤šæ™ºèƒ½ä½“å®šä¹‰ ====================

    # å†³ç­–çº§åˆ«çš„agentï¼Œå†³å®šä½¿ç”¨å“ªäº›agentï¼Œä»¥åŠå®ƒä»¬çš„è¿è¡Œé¡ºåº
PlannerAssistant = Assistants.create(
    model="qwen-plus",
    name='èº«ä½“å¼‚å¸¸åˆ†ææµç¨‹ç¼–æ’æœºå™¨äºº',
    description='ä½ æ˜¯èº«ä½“å¼‚å¸¸åˆ†æå›¢é˜Ÿçš„leaderï¼Œä½ éœ€è¦æ ¹æ®ç”¨æˆ·æä¾›çš„ç»“æ„åŒ–èº«ä½“æ•°æ®ï¼Œå†³å®šè¦ä»¥æ€æ ·çš„é¡ºåºå»ä½¿ç”¨è¿™äº›assistant',
    instructions="""ä½ çš„å›¢é˜Ÿä¸­æœ‰ä»¥ä¸‹assistantï¼š
    UserDataAnalysisAssistantï¼šå¯ä»¥åˆ†æç”¨æˆ·æä¾›çš„ç»“æ„åŒ–èº«ä½“æ•°æ®ï¼ˆåŒ…æ‹¬ä½“æˆåˆ†ã€ä½“æ€ã€å›´åº¦ç­‰ï¼‰ï¼Œæå–å…³é”®æŒ‡æ ‡ï¼›
    KnowledgeQueryAssistantï¼šå¯ä»¥æŸ¥è¯¢èº«ä½“å¼‚å¸¸åˆ¤æ–­å†³ç­–æ ‘çŸ¥è¯†åº“ï¼Œè·å–ä½“æˆåˆ†å’Œä½“æ€ç›¸å…³çš„åˆ¤æ–­è§„åˆ™ï¼›
    AbnormalityAnalysisAssistantï¼šå¯ä»¥åŸºäºç”¨æˆ·æ•°æ®å’Œå†³ç­–æ ‘è§„åˆ™ï¼Œè¿›è¡Œä½“æˆåˆ†å¼‚å¸¸åˆ†æï¼ˆ1ä¸ªï¼‰å’Œä½“æ€å¼‚å¸¸åˆ†æï¼ˆæœ€å¤š4ä¸ªï¼‰ï¼›
    ChatAssistantï¼šå¦‚æœç”¨æˆ·çš„é—®é¢˜ä¸æ˜¯èº«ä½“æ•°æ®åˆ†æç›¸å…³ï¼Œåˆ™è°ƒç”¨è¯¥assistantã€‚
    
    å¯¹äºç»“æ„åŒ–èº«ä½“æ•°æ®åˆ†æé—®é¢˜ï¼Œæ¨èçš„æµç¨‹æ˜¯ï¼š["UserDataAnalysisAssistant", "KnowledgeQueryAssistant", "AbnormalityAnalysisAssistant"]
    å¯¹äºéèº«ä½“æ•°æ®åˆ†æé—®é¢˜ï¼š["ChatAssistant"]
    
    ä½ çš„è¿”å›å½¢å¼å¿…é¡»æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œä¸èƒ½è¿”å›å…¶å®ƒä¿¡æ¯ã€‚åˆ—è¡¨ä¸­çš„å…ƒç´ åªèƒ½ä¸ºä¸Šè¿°çš„assistantåç§°ã€‚"""
)

# åŠŸèƒ½æ˜¯å›å¤æ—¥å¸¸é—®é¢˜ã€‚å¯¹äºæ—¥å¸¸é—®é¢˜æ¥è¯´ï¼Œå¯ä»¥ä½¿ç”¨ä»·æ ¼è¾ƒä¸ºä½å»‰çš„æ¨¡å‹ä½œä¸ºagentçš„åŸºåº§
ChatAssistant = Assistants.create(
    model="qwen-turbo",
    name='å›ç­”æ—¥å¸¸é—®é¢˜çš„æœºå™¨äºº',
    description='ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè§£ç­”ç”¨æˆ·çš„é—®é¢˜',
    instructions='è¯·ç¤¼è²Œåœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜'
)

# ç”¨æˆ·æ•°æ®åˆ†æåŠ©æ‰‹
UserDataAnalysisAssistant = Assistants.create(
    model="qwen-plus",
    name='ç”¨æˆ·èº«ä½“æ•°æ®åˆ†ææœºå™¨äºº',
    description='ä¸€ä¸ªä¸“ä¸šçš„èº«ä½“æ•°æ®åˆ†æåŠ©æ‰‹ï¼Œèƒ½å¤Ÿåˆ†æç”¨æˆ·æä¾›çš„ç»“æ„åŒ–èº«ä½“æŒ‡æ ‡æ•°æ®',
    instructions='ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èº«ä½“æ•°æ®åˆ†æåŠ©æ‰‹ï¼Œä¸“é—¨è´Ÿè´£åˆ†æç”¨æˆ·æä¾›çš„èº«ä½“æ•°æ®ã€‚è¯·ä»”ç»†æå–ç”¨æˆ·æä¾›çš„ç»“æ„åŒ–èº«ä½“æ•°æ®ï¼ŒåŒ…æ‹¬ä½“æˆåˆ†æ•°æ®(mass_info)ã€å›´åº¦ä¿¡æ¯(girth_info)ã€ä½“æ€è¯„ä¼°(eval_info)ç­‰ï¼Œé‡ç‚¹å¯¹æ•°å€¼å’Œå·²æ˜ç¡®çš„å¼‚å¸¸è¿›è¡Œç»“æ„åŒ–å¤„ç†ï¼Œè€Œä¸è¦ä¸‹ç»“è®ºã€‚',
    tools=[
        {
            'type': 'function',
            'function': {
                'name': 'ç»“æ„åŒ–èº«ä½“æ•°æ®åˆ†æ',
                'description': 'æ ¹æ®ç”¨æˆ·æä¾›çš„ç»“æ„åŒ–èº«ä½“æ•°æ®ï¼Œæå–å…¨éƒ¨ä½“æˆåˆ†ã€ä½“æ€å’Œä½“å›´ç­‰å…³é”®æŒ‡æ ‡ä¿¡æ¯',
                'parameters': {
                    'type': 'object',
                    'properties': {
                        'user_data': {
                            'type': 'string',
                            'description': 'ç”¨æˆ·æä¾›çš„ç»“æ„åŒ–èº«ä½“æ•°æ®å†…å®¹ï¼ŒåŒ…å«mass_infoã€girth_infoã€eval_infoç­‰'
                        },
                    },
                    'required': ['user_data']},
            }
        }
    ]
)

# çŸ¥è¯†åº“æŸ¥è¯¢åŠ©æ‰‹
KnowledgeQueryAssistant = Assistants.create(
    model="qwen-plus",
    name='èº«ä½“å¼‚å¸¸å†³ç­–æ ‘æŸ¥è¯¢æœºå™¨äºº',
    description='ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ï¼Œèƒ½å¤ŸæŸ¥è¯¢èº«ä½“å¼‚å¸¸åˆ¤æ–­å†³ç­–æ ‘çŸ¥è¯†åº“è·å–ä½“æˆåˆ†å’Œä½“æ€å¼‚å¸¸ç›¸å…³åˆ¤æ–­è§„åˆ™',
    instructions='''ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èº«ä½“å¼‚å¸¸åˆ†æåŠ©æ‰‹ï¼Œä¸“é—¨è´Ÿè´£æŸ¥è¯¢èº«ä½“å¼‚å¸¸åˆ¤æ–­å†³ç­–æ ‘çŸ¥è¯†åº“ã€‚

ã€é‡è¦ã€‘ï¼šä½ å¿…é¡»åˆ†åˆ«è°ƒç”¨ä¸¤ä¸ªå·¥å…·å‡½æ•°ï¼š
1. å…ˆè°ƒç”¨"ä½“æˆåˆ†å¼‚å¸¸å†³ç­–æ ‘æŸ¥è¯¢"å·¥å…·ï¼ŒæŸ¥è¯¢BMIã€ä½“è„‚ç‡ã€å»è„‚ä½“é‡ç­‰ä½“æˆåˆ†ç›¸å…³çš„åˆ¤æ–­è§„åˆ™
2. å†è°ƒç”¨"ä½“æ€å¼‚å¸¸å†³ç­–æ ‘æŸ¥è¯¢"å·¥å…·ï¼ŒæŸ¥è¯¢é«˜ä½è‚©ã€å¤´å‰å€¾ã€åœ†è‚©ã€å¤´ä¾§æ­ªã€éª¨ç›†å‰ç§»ç­‰ä½“æ€ç›¸å…³çš„åˆ¤æ–­è§„åˆ™

æ— è®ºç”¨æˆ·æ•°æ®ä¸­æ˜¯å¦åŒ…å«å…·ä½“çš„æ•°å€¼æŒ‡æ ‡ï¼Œéƒ½è¦è°ƒç”¨è¿™ä¸¤ä¸ªå·¥å…·å‡½æ•°æ¥è·å–å®Œæ•´çš„å†³ç­–æ ‘è§„åˆ™ã€‚è¿™æ ·å¯ä»¥ç¡®ä¿åç»­çš„å¼‚å¸¸åˆ†æèƒ½å¤Ÿè·å¾—å…¨é¢çš„åˆ¤æ–­ä¾æ®ã€‚''',
    tools=[
        {
            'type': 'function',
            'function': {
                'name': 'ä½“æˆåˆ†å¼‚å¸¸å†³ç­–æ ‘æŸ¥è¯¢',
                'description': 'æŸ¥è¯¢ä½“æˆåˆ†å¼‚å¸¸åˆ¤æ–­å†³ç­–æ ‘çŸ¥è¯†åº“è·å–ç›¸å…³çš„åˆ¤æ–­è§„åˆ™',
                'parameters': {
                    'type': 'object',
                    'properties': {
                        'query_text': {
                            'type': 'string',
                            'description': 'è¦æŸ¥è¯¢çš„ä½“æˆåˆ†æŒ‡æ ‡ç›¸å…³å†…å®¹ï¼Œå¦‚BMIã€ä½“è„‚ç‡ã€å»è„‚ä½“é‡æŒ‡æ•°ç­‰'
                        },
                        'knowledge_base_name': {
                            'type': 'string',
                            'description': 'æŒ‡å®šçš„çŸ¥è¯†åº“åç§°ï¼Œå¯é€‰å‚æ•°'
                        }
                    },
                    'required': ['query_text']},
            }
        },
        {
            'type': 'function',
            'function': {
                'name': 'ä½“æ€å¼‚å¸¸å†³ç­–æ ‘æŸ¥è¯¢',
                'description': 'æŸ¥è¯¢ä½“æ€å¼‚å¸¸åˆ¤æ–­å†³ç­–æ ‘çŸ¥è¯†åº“è·å–ç›¸å…³çš„åˆ¤æ–­è§„åˆ™',
                'parameters': {
                    'type': 'object',
                    'properties': {
                        'query_text': {
                            'type': 'string',
                            'description': 'è¦æŸ¥è¯¢çš„ä½“æ€æŒ‡æ ‡ç›¸å…³å†…å®¹ï¼Œå¦‚é«˜ä½è‚©ã€å¤´å‰å€¾ã€åœ†è‚©ã€è…¿å‹ç­‰'
                        },
                        'knowledge_base_name': {
                            'type': 'string',
                            'description': 'æŒ‡å®šçš„çŸ¥è¯†åº“åç§°ï¼Œå¯é€‰å‚æ•°'
                        }
                    },
                    'required': ['query_text']},
            }
        }
    ]
)

# èº«ä½“å¼‚å¸¸åˆ†æåŠ©æ‰‹
AbnormalityAnalysisAssistant = Assistants.create(
    model="qwen-plus",
    name='èº«ä½“å¼‚å¸¸åˆ†ææœºå™¨äºº',
    description='ä¸€ä¸ªä¸“ä¸šçš„èº«ä½“å¼‚å¸¸åˆ†æåŠ©æ‰‹ï¼Œèƒ½å¤ŸåŸºäºç”¨æˆ·æ•°æ®å’Œå†³ç­–æ ‘è§„åˆ™è¿›è¡Œä½“æˆåˆ†å’Œä½“æ€å¼‚å¸¸åˆ¤æ–­',
    instructions="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èº«ä½“å¼‚å¸¸åˆ†æä¸“å®¶ï¼Œä¸“é—¨è´Ÿè´£æ ¹æ®çŸ¥è¯†åº“ä¸­çš„å†³ç­–æ ‘è§„åˆ™å’Œç”¨æˆ·èº«ä½“æ•°æ®è¿›è¡Œç²¾ç¡®çš„å¼‚å¸¸åˆ¤æ–­ã€‚

ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¦æ±‚æ‰§è¡Œï¼š

ã€æ ¸å¿ƒè¦æ±‚ã€‘
1. å¿…é¡»ç”Ÿæˆ1ä¸ªä½“æˆåˆ†å¼‚å¸¸åˆ†æå’Œæœ€å¤š4ä¸ªä½“æ€å¼‚å¸¸åˆ†æ
2. ä¸¥æ ¼æŒ‰ç…§çŸ¥è¯†åº“å†³ç­–æ ‘è§„åˆ™è¿›è¡Œåˆ¤æ–­ï¼Œä¸å¾—æ“…è‡ªæ¨æµ‹
3. æ¯ä¸ªå¼‚å¸¸åˆ¤æ–­éƒ½å¿…é¡»å±•ç¤ºå®Œæ•´çš„å†³ç­–æµç¨‹
4. æŒ‰ç…§çŸ¥è¯†åº“ä¸­çš„ä¼˜å…ˆçº§æ’åºå¼‚å¸¸

ã€åˆ†ææµç¨‹ã€‘
1. è§£æç”¨æˆ·èº«ä½“æ•°æ®ï¼Œæå–å…³é”®æŒ‡æ ‡
2. è§£æçŸ¥è¯†åº“å†³ç­–æ ‘è§„åˆ™ï¼Œè¯†åˆ«åˆ¤æ–­æ¡ä»¶å’Œå¼‚å¸¸ç»“è®º
3. å°†ç”¨æˆ·æ•°æ®ä¸å†³ç­–æ ‘æ¡ä»¶è¿›è¡ŒåŒ¹é…
4. å±•ç¤ºè¯¦ç»†çš„åˆ¤æ–­è¿‡ç¨‹ï¼šæ¡ä»¶â†’ç”¨æˆ·æ•°æ®â†’åŒ¹é…ç»“æœâ†’ç»“è®º
5. æŒ‰ä¼˜å…ˆçº§æ’åºå¹¶è¾“å‡ºæœ€ç»ˆç»“è®º

ã€è¾“å‡ºæ ¼å¼è¦æ±‚ã€‘
å¯¹äºæ¯ä¸ªå¼‚å¸¸åˆ†æï¼Œå¿…é¡»åŒ…å«ï¼š
- åº”ç”¨çš„å†³ç­–è§„åˆ™æ¡ä»¶
- ç”¨æˆ·æ•°æ®å¦‚ä½•åŒ¹é…è¿™äº›æ¡ä»¶  
- å…·ä½“çš„åŒ¹é…è¿‡ç¨‹å’Œè®¡ç®—
- åŸºäºåŒ¹é…ç»“æœå¾—å‡ºçš„ç»“è®º
- å®Œæ•´çš„å†³ç­–æµç¨‹å±•ç¤º

ã€ä¸¥æ ¼é™åˆ¶ã€‘
- åªèƒ½åŸºäºçŸ¥è¯†åº“ä¸­å­˜åœ¨çš„å†³ç­–è§„åˆ™è¿›è¡Œåˆ¤æ–­
- ä¸èƒ½æ ¹æ®ç»éªŒæˆ–å¸¸è¯†åšå‡ºåˆ¤æ–­
- å¿…é¡»å±•ç¤ºä»æ¡ä»¶åˆ¤æ–­åˆ°ç»“è®ºçš„å®Œæ•´è¿‡ç¨‹
- å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³è§„åˆ™ï¼Œå¿…é¡»æ˜ç¡®è¯´æ˜æ— æ³•åˆ¤æ–­

ä½ çš„åˆ†æå¿…é¡»ä¸“ä¸šã€å‡†ç¡®ã€å¯è¿½æº¯ï¼Œç¡®ä¿æ¯ä¸ªç»“è®ºéƒ½æœ‰æ˜ç¡®çš„å†³ç­–ä¾æ®ã€‚""",
    tools=[
        {
            'type': 'function',
            'function': {
                'name': 'ä½“æˆåˆ†å¼‚å¸¸åˆ†æ',
                'description': 'åŸºäºç”¨æˆ·ä½“æˆåˆ†æ•°æ®å’Œå†³ç­–æ ‘è§„åˆ™ï¼Œè¿›è¡Œä¸¥æ ¼çš„ä½“æˆåˆ†å¼‚å¸¸åˆ†æå’Œåˆ¤æ–­ï¼Œå¿…é¡»ç”Ÿæˆ1ä¸ªä½“æˆåˆ†å¼‚å¸¸åˆ†æ',
                'parameters': {
                    'type': 'object',
                    'properties': {
                        'user_data': {
                            'type': 'string',
                            'description': 'ç”¨æˆ·ä½“æˆåˆ†æ•°æ®åˆ†æç»“æœï¼ŒåŒ…å«BMIã€ä½“è„‚ç‡ã€å»è„‚ä½“é‡ç­‰å…³é”®æŒ‡æ ‡'
                        },
                        'decision_rules': {
                            'type': 'string',
                            'description': 'ä»å†³ç­–æ ‘çŸ¥è¯†åº“è·å¾—çš„ä½“æˆåˆ†ç›¸å…³åˆ¤æ–­è§„åˆ™ï¼ŒåŒ…å«å¼‚å¸¸ç»“è®ºå’Œåˆ¤æ–­æµç¨‹'
                        }
                    },
                    'required': ['user_data', 'decision_rules']},
            }
        },
        {
            'type': 'function',
            'function': {
                'name': 'ä½“æ€å¼‚å¸¸åˆ†æ',
                'description': 'åŸºäºç”¨æˆ·ä½“æ€æ•°æ®å’Œå†³ç­–æ ‘è§„åˆ™ï¼Œè¿›è¡Œä¸¥æ ¼çš„ä½“æ€å¼‚å¸¸åˆ†æå’Œåˆ¤æ–­ï¼Œæœ€å¤šç”Ÿæˆ4ä¸ªä½“æ€å¼‚å¸¸åˆ†æ',
                'parameters': {
                    'type': 'object',
                    'properties': {
                        'user_data': {
                            'type': 'string',
                            'description': 'ç”¨æˆ·ä½“æ€æ•°æ®åˆ†æç»“æœï¼ŒåŒ…å«é«˜ä½è‚©ã€å¤´å‰å€¾ã€åœ†è‚©ç­‰ä½“æ€æŒ‡æ ‡'
                        },
                        'decision_rules': {
                            'type': 'string',
                            'description': 'ä»å†³ç­–æ ‘çŸ¥è¯†åº“è·å¾—çš„ä½“æ€ç›¸å…³åˆ¤æ–­è§„åˆ™ï¼ŒåŒ…å«å¼‚å¸¸ç»“è®ºå’Œåˆ¤æ–­æµç¨‹'
                        }
                    },
                    'required': ['user_data', 'decision_rules']},
            }
        }
    ]
)

# åœ¨Multi Agentåœºæ™¯ä¸‹ï¼Œå®šä¹‰ä¸€ä¸ªç”¨äºæ€»ç»“çš„Agentï¼Œè¯¥Agentä¼šæ ¹æ®ç”¨æˆ·çš„é—®é¢˜ä¸ä¹‹å‰Agentè¾“å‡ºçš„å‚è€ƒä¿¡æ¯ï¼Œå…¨é¢ã€å®Œæ•´åœ°å›ç­”ç”¨æˆ·é—®é¢˜
SummaryAssistant = Assistants.create(
    model="qwen-plus",
    name='èº«ä½“å¼‚å¸¸æ€»ç»“æœºå™¨äºº',
    description='ä¸€ä¸ªä¸“ä¸šçš„èº«ä½“å¼‚å¸¸åˆ†æåŠ©æ‰‹ï¼Œæ ¹æ®ç”¨æˆ·çš„èº«ä½“æ•°æ®ä¸å„ä¸ªåˆ†æé˜¶æ®µçš„å‚è€ƒä¿¡æ¯ï¼Œæä¾›å…¨é¢ã€å®Œæ•´çš„å¼‚å¸¸åˆ†æç»“è®º',
    instructions="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„èº«ä½“å¼‚å¸¸åˆ†ææ€»ç»“ä¸“å®¶ï¼Œè´Ÿè´£åŸºäºå¤šæ™ºèƒ½ä½“åˆ†æç»“æœæä¾›æœ€ç»ˆçš„ç»¼åˆæŠ¥å‘Šã€‚

ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¦æ±‚è¿›è¡Œæ€»ç»“ï¼š

ã€æ ¸å¿ƒä»»åŠ¡ã€‘
1. æ€»ç»“åˆ†æå‡º1ä¸ªä½“æˆåˆ†å¼‚å¸¸å’Œæœ€å¤š4ä¸ªä½“æ€å¼‚å¸¸
2. ä¸¥æ ¼æŒ‰ç…§çŸ¥è¯†åº“ä¸­çš„ä¼˜å…ˆçº§æ’åºå¼‚å¸¸ç»“è®º
3. å±•ç¤ºå®Œæ•´çš„å†³ç­–åˆ¤æ–­æµç¨‹
4. æä¾›åŸºäºå†³ç­–æ ‘è§„åˆ™çš„ä¸“ä¸šç»“è®º

ã€æ€»ç»“æ ¼å¼è¦æ±‚ã€‘
## èº«ä½“å¼‚å¸¸åˆ†ææŠ¥å‘Š

### ä¸€ã€ä½“æˆåˆ†å¼‚å¸¸åˆ†æ
- **å¼‚å¸¸ç»“è®º**: [åŸºäºå†³ç­–æ ‘è§„åˆ™å¾—å‡ºçš„ç»“è®º]
- **åˆ¤æ–­ä¾æ®**: [å…·ä½“çš„å†³ç­–è§„åˆ™æ¡ä»¶]
- **æ•°æ®åŒ¹é…**: [ç”¨æˆ·æ•°æ®å¦‚ä½•æ»¡è¶³æ¡ä»¶]
- **å†³ç­–æµç¨‹**: [ä»æ¡ä»¶åˆ°ç»“è®ºçš„å®Œæ•´è¿‡ç¨‹]
- **ä¼˜å…ˆçº§**: [çŸ¥è¯†åº“ä¸­çš„ä¼˜å…ˆçº§]

### äºŒã€ä½“æ€å¼‚å¸¸åˆ†æï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
1. **[ä¼˜å…ˆçº§1] å¼‚å¸¸åç§°**
   - åˆ¤æ–­ä¾æ®: [å†³ç­–è§„åˆ™]
   - æ•°æ®åŒ¹é…: [ç”¨æˆ·æ•°æ®åŒ¹é…æƒ…å†µ]
   - å†³ç­–æµç¨‹: [åˆ¤æ–­è¿‡ç¨‹]

2. **[ä¼˜å…ˆçº§X] å¼‚å¸¸åç§°**
   - åˆ¤æ–­ä¾æ®: [å†³ç­–è§„åˆ™]
   - æ•°æ®åŒ¹é…: [ç”¨æˆ·æ•°æ®åŒ¹é…æƒ…å†µ]
   - å†³ç­–æµç¨‹: [åˆ¤æ–­è¿‡ç¨‹]

### ä¸‰ã€ç»¼åˆç»“è®º
- åŸºäºçŸ¥è¯†åº“å†³ç­–æ ‘è§„åˆ™ï¼Œç”¨æˆ·å­˜åœ¨Xé¡¹å¼‚å¸¸
- æŒ‰ä¼˜å…ˆçº§æ’åºçš„é£é™©ç­‰çº§
- å»ºè®®é‡‡å–çš„æªæ–½

ã€ä¸¥æ ¼è¦æ±‚ã€‘
- åªèƒ½åŸºäºçŸ¥è¯†åº“å†³ç­–æ ‘è§„åˆ™å¾—å‡ºç»“è®º
- å¿…é¡»å±•ç¤ºæ¯ä¸ªå¼‚å¸¸çš„å®Œæ•´åˆ¤æ–­æµç¨‹
- ä¸¥æ ¼æŒ‰ç…§ä¼˜å…ˆçº§æ’åºï¼ˆæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
- å¦‚æœæŸç±»å¼‚å¸¸æœªå‘ç°ï¼Œéœ€æ˜ç¡®è¯´æ˜
- æ¯ä¸ªç»“è®ºéƒ½è¦æœ‰æ˜ç¡®çš„å†³ç­–ä¾æ®

ã€å…è´£å£°æ˜ã€‘
æ­¤åˆ†æç»“æœä¸¥æ ¼åŸºäºçŸ¥è¯†åº“å†³ç­–æ ‘è§„åˆ™ï¼Œä»…ä¾›å‚è€ƒï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šçš„å¥åº·é¡¾é—®æˆ–åŒ»ç”Ÿè·å–å‡†ç¡®è¯Šæ–­ã€‚"""
)

# å°†å·¥å…·å‡½æ•°çš„nameæ˜ å°„åˆ°å‡½æ•°æœ¬ä½“
function_mapper = {
    "ç»“æ„åŒ–èº«ä½“æ•°æ®åˆ†æ": MedicalAnalysis.analyze_symptoms,
    "ä½“æˆåˆ†å¼‚å¸¸å†³ç­–æ ‘æŸ¥è¯¢": MedicalAnalysis.query_medical_knowledge,
    "ä½“æ€å¼‚å¸¸å†³ç­–æ ‘æŸ¥è¯¢": MedicalAnalysis.query_medical_knowledge,
    "ä½“æˆåˆ†å¼‚å¸¸åˆ†æ": HealthAssessment.body_composition_analysis,
    "ä½“æ€å¼‚å¸¸åˆ†æ": HealthAssessment.posture_analysis
}

# å°†Agentçš„nameæ˜ å°„åˆ°Agentæœ¬ä½“
assistant_mapper = {
    "ChatAssistant": ChatAssistant,
    "UserDataAnalysisAssistant": UserDataAnalysisAssistant,
    "KnowledgeQueryAssistant": KnowledgeQueryAssistant,
    "AbnormalityAnalysisAssistant": AbnormalityAnalysisAssistant
}

# ==================== Agentå¤„ç†å‡½æ•° ====================

def get_agent_response(assistant, message='', return_tool_output=False):
    """è¾“å…¥messageä¿¡æ¯ï¼Œè¾“å‡ºä¸ºæŒ‡å®šAgentçš„å›å¤"""
    #print(f"Query: {message}")
    thread = Threads.create()
    message = Messages.create(thread.id, content=message)
    run = Runs.create(thread.id, assistant_id=assistant.id)
    run_status = Runs.wait(run.id, thread_id=thread.id)
    
    all_tool_output = ""  # å­˜å‚¨æ‰€æœ‰å·¥å…·è¾“å‡º
    print("run_status:",run_status)
    
    if run_status.status == 'failed':
        print('run failed:')
        return ("æŠ±æ­‰ï¼Œå¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯", all_tool_output) if return_tool_output else "æŠ±æ­‰ï¼Œå¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯"
    
    # ğŸ”¥ å¾ªç¯å¤„ç†å¤šä¸ªå·¥å…·è°ƒç”¨
    while run_status.required_action:
        tool_calls = run_status.required_action.submit_tool_outputs.tool_calls
        tool_outputs = []
        
        # å¤„ç†å¤šä¸ªå·¥å…·è°ƒç”¨
        for tool_call in tool_calls:
            f = tool_call.function
            func_name = f['name']  
            param = json.loads(f['arguments'])
            print(f"è°ƒç”¨å·¥å…·: {func_name}")
        
            if func_name in function_mapper:
                # å¦‚æœæ˜¯å†³ç­–æ ‘æŸ¥è¯¢ï¼Œæ·»åŠ çŸ¥è¯†åº“å‚æ•°
                if func_name in ["ä½“æˆåˆ†å¼‚å¸¸å†³ç­–æ ‘æŸ¥è¯¢", "ä½“æ€å¼‚å¸¸å†³ç­–æ ‘æŸ¥è¯¢"] and 'knowledge_base_name' not in param:
                    # ä»å…¨å±€å˜é‡æˆ–ä¼ é€’çš„å‚æ•°ä¸­è·å–çŸ¥è¯†åº“åç§°
                    import inspect
                    frame = inspect.currentframe()
                    try:
                        # å°è¯•ä»è°ƒç”¨å †æ ˆä¸­è·å–knowledge_baseå‚æ•°
                        caller_locals = frame.f_back.f_back.f_locals
                        if 'knowledge_base' in caller_locals:
                            param['knowledge_base_name'] = caller_locals['knowledge_base']
                    finally:
                        del frame
                
                try:
                    output = function_mapper[func_name](**param)
                    # ç¡®ä¿è¾“å‡ºä¸ä¸ºç©º
                    if output is None:
                        output = '{"error": "å·¥å…·å‡½æ•°è¿”å›ç©ºå€¼"}'
                    elif not isinstance(output, str):
                        output = str(output)
                    
                    all_tool_output += f"{func_name}: {output}\n"  # ç´¯ç§¯å·¥å…·è¾“å‡º
                    print(f"å·¥å…· {func_name} æ‰§è¡ŒæˆåŠŸ")
                except Exception as e:
                    print(f"å·¥å…·å‡½æ•°æ‰§è¡Œå¤±è´¥ {func_name}: {e}")
                    output = f'{{"error": "å·¥å…·å‡½æ•°æ‰§è¡Œå¤±è´¥: {str(e)}"}}'
                    all_tool_output += f"{func_name}: {output}\n"
            else:    
                output = '{"error": "æœªçŸ¥çš„å·¥å…·å‡½æ•°"}'
                print(f"æœªçŸ¥å·¥å…·å‡½æ•°: {func_name}")
            
            tool_outputs.append({
                'tool_call_id': tool_call.id,
                'output': output
            })
        
        # æäº¤å·¥å…·è¾“å‡º
        run = Runs.submit_tool_outputs(run.id,
                                       thread_id=thread.id,
                                       tool_outputs=tool_outputs)
        run_status = Runs.wait(run.id, thread_id=thread.id)
        print(f"å·¥å…·è°ƒç”¨å®Œæˆï¼ŒçŠ¶æ€: {run_status.status}")
    
    # è·å–æœ€ç»ˆå“åº”
    run_status = Runs.get(run.id, thread_id=thread.id)
    msgs = Messages.list(thread.id)
    response = msgs['data'][0]['content'][0]['text']['value']
    
    if return_tool_output:
        return response, all_tool_output
    else:
        return response

def get_multi_agent_response_internal(query, knowledge_base=None):
    """è·å¾—Multi Agentçš„å›å¤çš„å†…éƒ¨å‡½æ•°"""
    if len(query) == 0:
        return "è¯·è¾“å…¥æ‚¨çš„èº«ä½“æ•°æ®æˆ–é—®é¢˜", ""
    
    collected_knowledge_chunks = ""  # æ”¶é›†çŸ¥è¯†åº“å¬å›ä¿¡æ¯
    
    try:
        # è·å–Agentçš„è¿è¡Œé¡ºåº
        # assistant_order = get_agent_response(PlannerAssistant, query)
        # print("assistant_order", assistant_order)
        
        # å®‰å…¨åœ°è§£æAssistanté¡ºåº
        # try:
        #     if isinstance(assistant_order, str):
        #         # å°è¯•ä¸åŒçš„è§£ææ–¹æ³•
        #         assistant_order = assistant_order.strip()
        #         if assistant_order.startswith('[') and assistant_order.endswith(']'):
        #             # å¦‚æœæ˜¯åˆ—è¡¨æ ¼å¼
        #             order_stk = ast.literal_eval(assistant_order)
        #         elif assistant_order.startswith('{') and assistant_order.endswith('}'):
        #             # å¦‚æœæ˜¯JSONæ ¼å¼
        #             order_data = json.loads(assistant_order)
        #             order_stk = order_data if isinstance(order_data, list) else [assistant_order]
        #         else:
        #             # å°è¯•ä»æ–‡æœ¬ä¸­æå–åˆ—è¡¨
        #             import re
        #             list_match = re.search(r'\[(.*?)\]', assistant_order)
        #             if list_match:
        #                 order_stk = ast.literal_eval('[' + list_match.group(1) + ']')
        #             else:
        #                 # é»˜è®¤å¤„ç†
        #                 order_stk = ["UserDataAnalysisAssistant", "KnowledgeQueryAssistant", "AbnormalityAnalysisAssistant"]
        #     else:
        #         order_stk = assistant_order if isinstance(assistant_order, list) else [str(assistant_order)]
        # except Exception as e:
        #     print(f"è§£æassistant_orderå¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤é¡ºåº")
        #     order_stk = ["UserDataAnalysisAssistant", "KnowledgeQueryAssistant", "AbnormalityAnalysisAssistant"]
        order_stk = ["UserDataAnalysisAssistant", "KnowledgeQueryAssistant", "AbnormalityAnalysisAssistant"]
        
        # æå–ç”¨æˆ·èº«ä½“æ•°æ®ï¼ˆä»åŸå§‹queryä¸­ï¼‰
        user_body_data = ""
        if "è¯·åˆ†æä»¥ä¸‹èº«ä½“æ•°æ®" in query:
            # æå–JSONæ•°æ®éƒ¨åˆ†
            import re
            json_match = re.search(r'ï¼š(\{.*\})$', query)
            if json_match:
                user_body_data = json_match.group(1)
            else:
                user_body_data = query
        else:
            user_body_data = query
        
        Agent_Message = ""
        previous_responses = {}  # å­˜å‚¨å„ä¸ªAgentçš„å“åº”
        
        # ä¾æ¬¡è¿è¡ŒAgent
        for i in range(len(order_stk)):
            assistant_name = order_stk[i]
            cur_assistant = assistant_mapper[assistant_name]
            
            # ä¸ºä¸åŒçš„Assistantå®šåˆ¶ä¸“é—¨çš„æŸ¥è¯¢å†…å®¹
            if assistant_name == "UserDataAnalysisAssistant":
                # æ•°æ®åˆ†æAssistantï¼šä¸“æ³¨äºç»“æ„åŒ–å¤„ç†æ‰€æœ‰èº«ä½“æ•°æ®
                cur_query = f"è¯·å¯¹ä»¥ä¸‹èº«ä½“æ•°æ®è¿›è¡Œå…¨é¢çš„ç»“æ„åŒ–åˆ†æå’Œæå–ï¼ŒåŒ…æ‹¬æ‰€æœ‰ä½“æˆåˆ†æŒ‡æ ‡ã€ä½“æ€æŒ‡æ ‡ã€å›´åº¦ä¿¡æ¯ç­‰ï¼š{user_body_data}"
                
            elif assistant_name == "KnowledgeQueryAssistant":
                # çŸ¥è¯†åº“æŸ¥è¯¢Assistantï¼šåŸºäºå‰é¢çš„æ•°æ®åˆ†æç»“æœæŸ¥è¯¢å†³ç­–æ ‘è§„åˆ™
                user_analysis = previous_responses.get("UserDataAnalysisAssistant", "")
                cur_query = f"åŸºäºä»¥ä¸‹ç”¨æˆ·èº«ä½“æ•°æ®åˆ†æç»“æœï¼Œè¯·åˆ†åˆ«æŸ¥è¯¢ä½“æˆåˆ†å¼‚å¸¸å’Œä½“æ€å¼‚å¸¸çš„ç›¸å…³å†³ç­–æ ‘åˆ¤æ–­è§„åˆ™ã€‚ç”¨æˆ·æ•°æ®åˆ†æï¼š{user_analysis}ã€‚è¯·é‡ç‚¹æŸ¥è¯¢BMIã€ä½“è„‚ç‡ã€å»è„‚ä½“é‡ã€é«˜ä½è‚©ã€å¤´å‰å€¾ã€åœ†è‚©ç­‰ç›¸å…³çš„å¼‚å¸¸åˆ¤æ–­å†³ç­–æ ‘è§„åˆ™ã€‚"
                
            elif assistant_name == "AbnormalityAnalysisAssistant":
                # å¼‚å¸¸åˆ†æAssistantï¼šåŸºäºæ•°æ®å’Œå†³ç­–æ ‘è§„åˆ™ç”Ÿæˆå¼‚å¸¸åˆ†æ
                user_analysis = previous_responses.get("UserDataAnalysisAssistant", "")
                knowledge_rules = previous_responses.get("KnowledgeQueryAssistant", "")
                cur_query = f"è¯·åŸºäºç”¨æˆ·èº«ä½“æ•°æ®å’Œå†³ç­–æ ‘è§„åˆ™ï¼Œä¸¥æ ¼æŒ‰ç…§çŸ¥è¯†åº“è§„åˆ™ç”Ÿæˆ1ä¸ªä½“æˆåˆ†å¼‚å¸¸åˆ†æå’Œæœ€å¤š4ä¸ªä½“æ€å¼‚å¸¸åˆ†æï¼Œå¹¶æŒ‰ä¼˜å…ˆçº§æ’åºã€‚\n\nç”¨æˆ·æ•°æ®åˆ†æï¼š{user_analysis}\n\nå†³ç­–æ ‘è§„åˆ™ï¼š{knowledge_rules}"
                
            else:
                # å…¶ä»–Assistantä¿æŒåŸå§‹æŸ¥è¯¢
                cur_query = query
            
            print(f"{assistant_name}åŠ©æ‰‹å¼€å§‹å·¥ä½œï¼Œä¸“é—¨ä»»åŠ¡ï¼š{cur_query}")
            
            # å¦‚æœæ˜¯å†³ç­–æ ‘æŸ¥è¯¢åŠ©æ‰‹ï¼Œè·å–å·¥å…·è¾“å‡º
            if assistant_name == "KnowledgeQueryAssistant":
                response, tool_output = get_agent_response(cur_assistant, cur_query, return_tool_output=True)
                # è§£æå·¥å…·è¾“å‡ºä¸­çš„å†³ç­–æ ‘ä¿¡æ¯
                print("response", response)
                if tool_output:
                    try:
                        print(f"å·¥å…·è¾“å‡ºå†…å®¹: {tool_output}")  # è°ƒè¯•ä¿¡æ¯
                        # å¤„ç†å¤šä¸ªå·¥å…·è¾“å‡ºçš„æƒ…å†µ
                        if isinstance(tool_output, str) and tool_output.strip():
                            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¥æŸ¥æ‰¾å®Œæ•´çš„JSONå—
                            import re
                            
                            # æŸ¥æ‰¾æ‰€æœ‰çš„JSONå—ï¼ˆä»{å¼€å§‹åˆ°}ç»“æŸï¼‰
                            json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
                            json_matches = re.findall(json_pattern, tool_output, re.DOTALL)
                            
                            # å¦‚æœæ‰¾ä¸åˆ°å®Œæ•´çš„JSONï¼Œå°è¯•æŸ¥æ‰¾å¸¦å‰ç¼€çš„JSON
                            if not json_matches:
                                # æŸ¥æ‰¾å¸¦å‰ç¼€çš„è¾“å‡ºè¡Œ
                                lines = tool_output.strip().split('\n')
                                current_json = ""
                                in_json = False
                                brace_count = 0
                                
                                for line in lines:
                                    line = line.strip()
                                    if not line:
                                        continue
                                    
                                    # æ£€æŸ¥æ˜¯å¦æ˜¯å‰ç¼€è¡Œ
                                    if line.startswith(('ä½“æˆåˆ†å¼‚å¸¸å†³ç­–æ ‘æŸ¥è¯¢:', 'ä½“æ€å¼‚å¸¸å†³ç­–æ ‘æŸ¥è¯¢:')):
                                        # æå–å‰ç¼€åçš„å†…å®¹
                                        if ':' in line:
                                            prefix, content = line.split(':', 1)
                                            content = content.strip()
                                            if content.startswith('{'):
                                                current_json = content
                                                in_json = True
                                                brace_count = content.count('{') - content.count('}')
                                                if brace_count == 0:
                                                    json_matches.append(current_json)
                                                    current_json = ""
                                                    in_json = False
                                    elif in_json:
                                        # ç»§ç»­æ”¶é›†JSONå†…å®¹
                                        current_json += " " + line
                                        brace_count += line.count('{') - line.count('}')
                                        if brace_count == 0:
                                            json_matches.append(current_json)
                                            current_json = ""
                                            in_json = False
                                    elif line.startswith('{'):
                                        # ç›´æ¥çš„JSONå¼€å§‹
                                        current_json = line
                                        in_json = True
                                        brace_count = line.count('{') - line.count('}')
                                        if brace_count == 0:
                                            json_matches.append(current_json)
                                            current_json = ""
                                            in_json = False
                            
                            # è§£ææ‰¾åˆ°çš„JSONå—
                            for json_str in json_matches:
                                try:
                                    print(f"å°è¯•è§£æJSON: {repr(json_str[:100])}")  # è°ƒè¯•ä¿¡æ¯
                                    
                                    # å°è¯•è§£æJSON
                                    try:
                                        kb_data = json.loads(json_str)
                                    except json.JSONDecodeError:
                                        # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ast.literal_evalè§£æPythonå­—å…¸æ ¼å¼
                                        try:
                                            kb_data = ast.literal_eval(json_str)
                                        except (ValueError, SyntaxError) as e:
                                            print(f"æ— æ³•è§£æJSON/å­—å…¸æ ¼å¼: {e}")
                                            continue
                                    
                                    if isinstance(kb_data, dict) and "retrieved_chunks" in kb_data:
                                        chunks = kb_data["retrieved_chunks"]
                                        query_type = kb_data.get("query_type", "å†³ç­–æ ‘æŸ¥è¯¢")
                                        collected_knowledge_chunks += f"### {query_type}ç»“æœï¼š\n"
                                        for chunk in chunks[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                                            collected_knowledge_chunks += f"## {chunk.get('rule_id', 'N/A')}:\n{chunk.get('content', '')}\nç½®ä¿¡åº¦: {chunk.get('confidence_score', 'N/A')}\n\n"
                                except Exception as je:
                                    print(f"JSONè§£æé”™è¯¯: {je}")
                                    print(f"åŸå§‹JSONå­—ç¬¦ä¸²: {repr(json_str)}")
                                    continue
                            
                            # å¦‚æœæ²¡æœ‰æˆåŠŸè§£æä»»ä½•JSONï¼Œä½†æœ‰å·¥å…·è¾“å‡ºï¼Œæ˜¾ç¤ºåŸå§‹è¾“å‡º
                            if not collected_knowledge_chunks and tool_output.strip():
                                collected_knowledge_chunks += f"åŸå§‹å·¥å…·è¾“å‡º: {tool_output}...\n"
                                
                    except Exception as e:
                        print(f"è§£æå†³ç­–æ ‘å·¥å…·è¾“å‡ºå¤±è´¥: {e}")
                        # å¦‚æœè§£æå¤±è´¥ï¼Œç›´æ¥æ˜¾ç¤ºåŸå§‹è¾“å‡º
                        if tool_output and tool_output.strip():
                            collected_knowledge_chunks += f"å·¥å…·è¾“å‡ºè§£æå¤±è´¥ï¼ŒåŸå§‹å†…å®¹: {tool_output[:500]}...\n"
                        
                # å¦‚æœè¿˜æ²¡æœ‰è·å–åˆ°å†³ç­–æ ‘ä¿¡æ¯ï¼Œå°è¯•ç›´æ¥æŸ¥è¯¢ä½“æˆåˆ†å’Œä½“æ€å¼‚å¸¸
                if not collected_knowledge_chunks and knowledge_base:
                    try:
                        from tools import MedicalAnalysis
                        # æŸ¥è¯¢ä½“æˆåˆ†å¼‚å¸¸
                        composition_result = MedicalAnalysis.query_medical_knowledge("ä½“æˆåˆ†å¼‚å¸¸ BMI ä½“è„‚ç‡", knowledge_base)
                        composition_data = json.loads(composition_result)
                        if "retrieved_chunks" in composition_data:
                            chunks = composition_data["retrieved_chunks"]
                            collected_knowledge_chunks += "### ä½“æˆåˆ†å¼‚å¸¸ç›¸å…³è§„åˆ™ï¼š\n"
                            for chunk in chunks[:3]:  # è·å–å‰3ä¸ªä½“æˆåˆ†ç›¸å…³
                                collected_knowledge_chunks += f"## {chunk.get('rule_id', 'N/A')}:\n{chunk.get('content', '')}\nç½®ä¿¡åº¦: {chunk.get('confidence_score', 'N/A')}\n\n"
                        
                        # æŸ¥è¯¢ä½“æ€å¼‚å¸¸
                        posture_result = MedicalAnalysis.query_medical_knowledge("ä½“æ€å¼‚å¸¸ é«˜ä½è‚© å¤´å‰å€¾ åœ†è‚©", knowledge_base)
                        posture_data = json.loads(posture_result)
                        if "retrieved_chunks" in posture_data:
                            chunks = posture_data["retrieved_chunks"]
                            collected_knowledge_chunks += "### ä½“æ€å¼‚å¸¸ç›¸å…³è§„åˆ™ï¼š\n"
                            for chunk in chunks[:4]:  # è·å–å‰4ä¸ªä½“æ€ç›¸å…³
                                collected_knowledge_chunks += f"## {chunk.get('rule_id', 'N/A')}:\n{chunk.get('content', '')}\nç½®ä¿¡åº¦: {chunk.get('confidence_score', 'N/A')}\n\n"
                    except Exception as e:
                        print(f"ç›´æ¥æŸ¥è¯¢å†³ç­–æ ‘çŸ¥è¯†åº“å¤±è´¥: {e}")
            else:
                response = get_agent_response(cur_assistant, cur_query)
            
            # å­˜å‚¨å½“å‰Assistantçš„å“åº”
            previous_responses[assistant_name] = response
            Agent_Message += f"*{assistant_name}*çš„å›å¤ä¸ºï¼š{response}\n\n"
            
            # å¦‚æœå½“å‰Agentä¸ºæœ€åä¸€ä¸ªAgentï¼Œåˆ™å°†å…¶è¾“å‡ºä½œä¸ºMulti Agentçš„è¾“å‡º
            if i == len(order_stk)-1:
                # ä¸ºSummaryAssistantå‡†å¤‡æ›´è¯¦ç»†çš„æç¤º
                summary_prompt = f"""è¯·åŸºäºä»¥ä¸‹å¤šæ™ºèƒ½ä½“åˆ†æç»“æœï¼Œæä¾›æœ€ç»ˆçš„èº«ä½“å¼‚å¸¸åˆ†ææŠ¥å‘Šã€‚

åŸå§‹ç”¨æˆ·é—®é¢˜ï¼š{query}

å„é˜¶æ®µåˆ†æç»“æœï¼š
{Agent_Message}

è¯·æŒ‰ç…§æŒ‡å®šæ ¼å¼ç”ŸæˆåŒ…å«1ä¸ªä½“æˆåˆ†å¼‚å¸¸å’Œæœ€å¤š4ä¸ªä½“æ€å¼‚å¸¸çš„ç»¼åˆåˆ†ææŠ¥å‘Šï¼Œä¸¥æ ¼æŒ‰ç…§çŸ¥è¯†åº“ä¼˜å…ˆçº§æ’åºã€‚"""
                
                multi_agent_response = get_agent_response(SummaryAssistant, summary_prompt)
                
                # ç¡®ä¿æœ‰å¬å›æ–‡æœ¬æ®µæ˜¾ç¤º
                if not collected_knowledge_chunks:
                    if "KnowledgeQueryAssistant" in order_stk:
                        collected_knowledge_chunks = "å¤šæ™ºèƒ½ä½“æ¨¡å¼ï¼šå·²å®Œæˆèº«ä½“å¼‚å¸¸å†³ç­–æ ‘æŸ¥è¯¢ï¼Œä½†æœªæ£€ç´¢åˆ°è¶³å¤Ÿç›¸å…³çš„å†…å®¹ã€‚å»ºè®®æä¾›æ›´è¯¦ç»†çš„èº«ä½“æ•°æ®æˆ–å’¨è¯¢ä¸“ä¸šå¥åº·é¡¾é—®ã€‚"
                    else:
                        collected_knowledge_chunks = "å¤šæ™ºèƒ½ä½“æ¨¡å¼ï¼šæ­¤é—®é¢˜æœªæ¶‰åŠå†³ç­–æ ‘æŸ¥è¯¢ï¼Œå·²é€šè¿‡é€šç”¨é—®ç­”å¤„ç†ã€‚"
                
                return multi_agent_response, collected_knowledge_chunks
    
    except Exception as e:
        print(f"Multi-agent processing failed: {e}")
        # å…œåº•ç­–ç•¥ï¼Œå¦‚æœä¸Šè¿°ç¨‹åºè¿è¡Œå¤±è´¥ï¼Œåˆ™ç›´æ¥è°ƒç”¨ChatAssistant
        fallback_response = get_agent_response(ChatAssistant, query)
        return fallback_response, "å¤šæ™ºèƒ½ä½“æ¨¡å¼å‡ºé”™ï¼Œå·²åˆ‡æ¢åˆ°é€šç”¨é—®ç­”æ¨¡å¼"

# ==================== åŸæœ‰RAGå‡½æ•° ====================

def get_model_response(multi_modal_input,history,model,temperature,max_tokens,history_round,db_name,similarity_threshold,chunk_cnt):
    # prompt = multi_modal_input['text']
    prompt = history[-1][0]
    tmp_files = multi_modal_input['files']
    if os.path.exists(os.path.join("File",TMP_NAME)):
        db_name = TMP_NAME
    else:
        if tmp_files:
            create_tmp_kb(tmp_files)
            db_name = TMP_NAME
    # è·å–index
    print(f"prompt:{prompt},tmp_files:{tmp_files},db_name:{db_name}")
    try:
        dashscope_rerank = DashScopeRerank(
            top_n=chunk_cnt,
            return_documents=True,
            api_key="sk-51d30a5436ca433b8ff81e624a23dcac"
        )
        storage_context = StorageContext.from_defaults(
            persist_dir=os.path.join(DB_PATH,db_name)
        )
        index = load_index_from_storage(storage_context)
        print("indexè·å–å®Œæˆ")
        retriever_engine = index.as_retriever(
            similarity_top_k=20,
        )
        # è·å–chunk
        retrieve_chunk = retriever_engine.retrieve(prompt)
        print(f"åŸå§‹chunkä¸ºï¼š{retrieve_chunk}")
        try:
            results = dashscope_rerank.postprocess_nodes(retrieve_chunk, query_str=prompt)
            print(f"rerankæˆåŠŸï¼Œé‡æ’åçš„chunkä¸ºï¼š{results}")
        except:
            results = retrieve_chunk[:chunk_cnt]
            print(f"rerankå¤±è´¥ï¼Œchunkä¸ºï¼š{results}")
        chunk_text = ""
        chunk_show = ""
        for i in range(len(results)):
            if results[i].score >= similarity_threshold:
                chunk_text = chunk_text + f"## {i+1}:\n {results[i].text}\n"
                chunk_show = chunk_show + f"## {i+1}:\n {results[i].text}\nscore: {round(results[i].score,2)}\n"
        print(f"å·²è·å–chunkï¼š{chunk_text}")
        prompt_template = f"è¯·å‚è€ƒä»¥ä¸‹å†…å®¹ï¼š{chunk_text}ï¼Œä»¥åˆé€‚çš„è¯­æ°”å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š{prompt}ã€‚å¦‚æœå‚è€ƒå†…å®¹ä¸­æœ‰å›¾ç‰‡é“¾æ¥ä¹Ÿè¯·ç›´æ¥è¿”å›ã€‚"
    except Exception as e:
        print(f"å¼‚å¸¸ä¿¡æ¯ï¼š{e}")
        prompt_template = prompt
        chunk_show = ""
    history[-1][-1] = ""
    client = OpenAI(
        api_key="sk-51d30a5436ca433b8ff81e624a23dcac",
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    )                
    system_message = {'role': 'system', 'content': 'You are a helpful assistant.'}
    messages = []
    history_round = min(len(history),history_round)
    for i in range(history_round):
        messages.append({'role': 'user', 'content': history[-history_round+i][0]})
        messages.append({'role': 'assistant', 'content': history[-history_round+i][1]})
    messages.append({'role': 'user', 'content': prompt_template})
    messages = [system_message] + messages
    completion = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens,
        stream=True
        )
    assistant_response = ""
    for chunk in completion:
        assistant_response += chunk.choices[0].delta.content
        history[-1][-1] = assistant_response
        yield history,chunk_show

# ==================== ç»Ÿä¸€å“åº”å‡½æ•° ====================

def get_unified_response(multi_modal_input, history, mode, model, temperature, max_tokens, history_round, knowledge_base, similarity_threshold, chunk_cnt):
    """ç»Ÿä¸€çš„å“åº”å‡½æ•°ï¼Œæ”¯æŒRAGå’Œå¤šæ™ºèƒ½ä½“ä¸¤ç§æ¨¡å¼"""
    prompt = history[-1][0] if history else ""
    
    if mode == "multi_agent":
        # å¤šæ™ºèƒ½ä½“æ¨¡å¼
        try:
            response, knowledge_chunks = get_multi_agent_response_internal(prompt, knowledge_base)
            history[-1][-1] = response
            yield history, knowledge_chunks
        except Exception as e:
            print(f"å¤šæ™ºèƒ½ä½“æ¨¡å¼å¤±è´¥ï¼Œé™çº§åˆ°RAGæ¨¡å¼: {e}")
            # é™çº§åˆ°RAGæ¨¡å¼
            yield from get_model_response(multi_modal_input, history, model, temperature, max_tokens, history_round, knowledge_base, similarity_threshold, chunk_cnt)
    else:
        # RAGæ¨¡å¼
        yield from get_model_response(multi_modal_input, history, model, temperature, max_tokens, history_round, knowledge_base, similarity_threshold, chunk_cnt)
        
def test_body_analysis():
    """æµ‹è¯•èº«ä½“å¼‚å¸¸åˆ†æçš„å¤šæ™ºèƒ½ä½“æµç¨‹"""
    # ç¤ºä¾‹ç”¨æˆ·æ•°æ®
    user_body_data = {
        "mass_info":{
            "WT":{"l":25.8, "m":30.4, "h":35, "v":48.7, "status":3},
            "FFM":{"l":28.7, "m":31.9, "h":35.1, "v":4.9, "status":1},
            "TBW":{"l":37.285292814, "m":41.412983381000004, "h":45.540673948000006, "v":34.110146224000005, "status":1},
            "BMI":{"l":18.5, "m":22, "h":24, "v":23.4, "status":2},
            "PBF":{"l":10, "m":15, "h":20, "v":33.7, "status":3},
            "BMR":{"l":1432.4, "m":1591.6, "h":1750.8, "v":1413.1, "status":1},
            "WHR":{"l":0.8, "m":0.85, "h":0.9, "v":0.88, "status":2},
            "SM":{"l":28.440241599000004, "m":31.842184374000002, "h":35.289486386, "v":25.764046616, "status":1},
            "PROTEIN":{"l":9.979032140000001, "m":11.113013065, "h":12.24699399, "v":9.162565874, "status":1},
            "ICW":{"l":23.133210870000003, "m":25.718687379000002, "h":28.304163888, "v":21.092045205, "status":1},
            "ECW":{"l":14.152081944, "m":15.739655239000003, "h":17.327228534000003, "v":13.471693389, "status":1},
            "VAG":{"l":0.9, "h":10, "m":0, "v":9, "status":2}
        },
        "girth_info":{
            "left_calf":40.9, "right_calf":41.4, "neck":37.083999999999996,
            "waist":87.884, "hip":103.37800000000001, "left_upper_arm":28.194, "right_upper_arm":28.448
        },
        "eval_info":{
            "id":20985, "scan_id":"37123456789136-950b07ba-875a-41b7-8850-271bf5b79764",
            "high_low_shoulder":3.302, "head_slant":0, "head_forward":1.5,
            "left_leg_xo":179.5, "right_leg_xo":174.3, "pelvis_forward":176.5,
            "left_knee_check":183.6, "right_knee_check":175.9,
            "round_shoulder_left":7.7, "round_shoulder_right":19.6, "create_time":None
        },
        "eval_conclusion":{
            "head_slant":{"conclusion_key":"body-product.bsEval.headSlant.normal", "val":0},
            "high_low_shoulder":{"conclusion_key":"body-product.bsEval.highLowShoudler.left", "val":3.302},
            "head_forward":{"conclusion_key":"body-product.bsEval.headForward.head", "val":1.5},
            "round_shoulder_left":{"conclusion_key":"body-product.bsEval.roundShoulder.left", "val":7.7},
            "round_shoulder_right":{"conclusion_key":"body-product.bsEval.roundShoulder.right", "val":19.6},
            "is_new_math_tt":1
        },
        "shoulder_info":{
            "id":2192, "scan_id":"37123456789136-950b07ba-875a-41b7-8850-271bf5b79764",
            "result":1, "left_abduction":173.2, "right_abduction":181.1,
            "left_adduction":174.1, "right_adduction":171, "create_time":1724313842
        },
        "user_info":{"height":174, "age":30, "sex":1},
        "isForeign":"0", "unit":"imperial", "lang": "zh-CN"
    }
    
    # è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ä¾›åˆ†æä½¿ç”¨
    query = f"è¯·åˆ†æä»¥ä¸‹èº«ä½“æ•°æ®å¹¶ç”Ÿæˆ1ä¸ªä½“æˆåˆ†å¼‚å¸¸å’Œæœ€å¤š4ä¸ªä½“æ€å¼‚å¸¸åˆ†æï¼š{json.dumps(user_body_data, ensure_ascii=False)}"
    
    # è°ƒç”¨å¤šæ™ºèƒ½ä½“åˆ†æ
    try:
        response, knowledge_chunks = get_multi_agent_response_internal(query, "medical_kb")
        print("=== å¤šæ™ºèƒ½ä½“åˆ†æç»“æœ ===")
        print(f"åˆ†æç»“æœï¼š{response}")
        #print("\n=== çŸ¥è¯†åº“å¬å›ä¿¡æ¯ ===")
        #print(knowledge_chunks)
        return response, knowledge_chunks
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥ï¼š{e}")
        return None, None

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    test_body_analysis()