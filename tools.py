# å·¥å…·æ¨¡å—ï¼ŒåŒ…å«èº«ä½“å¼‚å¸¸åˆ†æç³»ç»Ÿä½¿ç”¨çš„å·¥å…·å‡½æ•°
import json
import os
import re
from llama_index.core import StorageContext, load_index_from_storage, Settings
from llama_index.embeddings.dashscope import (
    DashScopeEmbedding,
    DashScopeTextEmbeddingModels,
    DashScopeTextEmbeddingType,
)
from llama_index.postprocessor.dashscope_rerank import DashScopeRerank

# é…ç½®åµŒå…¥æ¨¡å‹
EMBED_MODEL = DashScopeEmbedding(
    model_name=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V2,
    text_type=DashScopeTextEmbeddingType.TEXT_TYPE_DOCUMENT,
    api_key="sk-51d30a5436ca433b8ff81e624a23dcac",
)
Settings.embed_model = EMBED_MODEL

class MedicalAnalysis:
    """èº«ä½“å¼‚å¸¸åˆ†æå·¥å…·ç±»"""
    
    @staticmethod
    def analyze_symptoms(user_data):
        """åˆ†æç”¨æˆ·æä¾›çš„ç»“æ„åŒ–èº«ä½“æ•°æ®"""
        result = {
            "body_data_analysis": {
                "input_data": user_data,
                "extracted_metrics": {},
                "body_composition": {},
                "posture_metrics": {},
                "data_categories": [],
                "analysis_summary": "æ­£åœ¨åˆ†æç»“æ„åŒ–èº«ä½“æ•°æ®..."
            }
        }
        
        try:
            # è§£æç»“æ„åŒ–æ•°æ®
            if isinstance(user_data, str):
                if user_data.strip().startswith('{'):
                    print("å¤„ç†ç»“æ„åŒ–æ•°æ®")
                    data = json.loads(user_data)
                else:
                    # å¤„ç†æ–‡æœ¬å½¢å¼çš„æ•°æ®
                    print("å¤„ç†æ–‡æœ¬æ•°æ®")
                    return MedicalAnalysis._analyze_text_data(user_data)
            else:
                data = user_data
            
            extracted_metrics = {}
            body_composition = {}
            posture_metrics = {}
            
            # è§£æç”¨æˆ·åŸºæœ¬ä¿¡æ¯
            if "user_info" in data:
                user_info = data["user_info"]
                extracted_metrics["èº«é«˜"] = user_info.get("height", "")
                extracted_metrics["å¹´é¾„"] = user_info.get("age", "")
                extracted_metrics["æ€§åˆ«"] = "ç”·æ€§" if user_info.get("sex") == 1 else "å¥³æ€§" if user_info.get("sex") == 0 or user_info.get("sex") == 2 else "æœªçŸ¥"
            
            # è§£æä½“æˆåˆ†æ•°æ®
            if "mass_info" in data:
                mass_info = data["mass_info"]
                for key, value in mass_info.items():
                    if isinstance(value, dict) and "v" in value:
                        body_composition[key] = {
                            "value": value["v"],
                            "low": value.get("l", ""),
                            "medium": value.get("m", ""),
                            "high": value.get("h", ""),
                            "status": value.get("status", "")
                        }
                        
                        # æå–å…³é”®æŒ‡æ ‡åˆ°extracted_metrics
                        if key == "BMI":
                            extracted_metrics["BMI"] = value["v"]
                        elif key == "PBF":
                            extracted_metrics["ä½“è„‚ç‡"] = value["v"]
                        elif key == "FFM":
                            extracted_metrics["å»è„‚ä½“é‡"] = value["v"]
                        elif key == "WT":
                            extracted_metrics["ä½“é‡"] = value["v"]
            
            # è§£æä½“æ€æ•°æ®
            if "eval_info" in data:
                eval_info = data["eval_info"]
                posture_metrics = {
                    "é«˜ä½è‚©": eval_info.get("high_low_shoulder", 0),
                    "å¤´å‰å€¾": eval_info.get("head_forward", 0),
                    "å¤´å€¾æ–œ": eval_info.get("head_slant", 0),
                    "å·¦åœ†è‚©": eval_info.get("round_shoulder_left", 0),
                    "å³åœ†è‚©": eval_info.get("round_shoulder_right", 0),
                    "å·¦è…¿å‹": eval_info.get("left_leg_xo", 0),
                    "å³è…¿å‹": eval_info.get("right_leg_xo", 0)
                }
                
                # æ·»åŠ åˆ°æå–æŒ‡æ ‡ä¸­
                for key, value in posture_metrics.items():
                    if value and value != 0:
                        extracted_metrics[key] = value
            
            # è§£æå›´åº¦ä¿¡æ¯
            if "girth_info" in data:
                girth_info = data["girth_info"]
                for key, value in girth_info.items():
                    if value:
                        extracted_metrics[f"å›´åº¦_{key}"] = value
            
            result["body_data_analysis"]["extracted_metrics"] = extracted_metrics
            result["body_data_analysis"]["body_composition"] = body_composition
            result["body_data_analysis"]["posture_metrics"] = posture_metrics
            
            # æ•°æ®åˆ†ç±»
            data_categories = []
            if body_composition:
                data_categories.append("ä½“æˆåˆ†æ•°æ®")
            if posture_metrics:
                data_categories.append("ä½“æ€æ•°æ®")
            if "girth_info" in data:
                data_categories.append("å›´åº¦æ•°æ®")
            if "user_info" in data:
                data_categories.append("åŸºæœ¬ä¿¡æ¯")
            
            result["body_data_analysis"]["data_categories"] = data_categories
            result["body_data_analysis"]["analysis_summary"] = (
                f"æˆåŠŸè§£æç»“æ„åŒ–èº«ä½“æ•°æ®ï¼ŒåŒ…å«ï¼š{len(body_composition)}é¡¹ä½“æˆåˆ†æŒ‡æ ‡ï¼Œ"
                f"{len([k for k, v in posture_metrics.items() if v != 0])}é¡¹ä½“æ€æŒ‡æ ‡ï¼Œ"
                f"å…±{len(extracted_metrics)}é¡¹å…³é”®æŒ‡æ ‡"
            )
            
        except Exception as e:
            result["body_data_analysis"]["error"] = f"è§£æç»“æ„åŒ–æ•°æ®æ—¶å‡ºé”™ï¼š{str(e)}"
            result["body_data_analysis"]["analysis_summary"] = "æ•°æ®è§£æå¤±è´¥"
        
        return json.dumps(result, ensure_ascii=False, indent=2)
    
    @staticmethod
    def _analyze_text_data(user_data):
        """åˆ†ææ–‡æœ¬å½¢å¼çš„èº«ä½“æ•°æ®ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰"""
        result = {
            "body_data_analysis": {
                "input_data": user_data,
                "extracted_metrics": {},
                "data_categories": [],
                "analysis_summary": f"æ¥æ”¶åˆ°èº«ä½“æ•°æ®ï¼š{user_data}ã€‚æ­£åœ¨è¿›è¡Œæ•°æ®è§£æå’Œåˆ†æ..."
            }
        }
        
        # è§£æèº«ä½“æ•°æ®
        data_lower = user_data.lower()
        extracted_metrics = {}
        
        # æå–æ€§åˆ«ä¿¡æ¯
        if "ç”·æ€§" in data_lower or "ç”·" in data_lower:
            extracted_metrics["æ€§åˆ«"] = "ç”·æ€§"
        elif "å¥³æ€§" in data_lower or "å¥³" in data_lower:
            extracted_metrics["æ€§åˆ«"] = "å¥³æ€§"
        
        # æå–BMIå€¼
        bmi_pattern = r"bmi[\s=ï¼š]*(\d+\.?\d*)"
        bmi_match = re.search(bmi_pattern, data_lower)
        if bmi_match:
            extracted_metrics["BMI"] = float(bmi_match.group(1))
        
        # æå–å»è„‚ä½“é‡æŒ‡æ•°
        ffmi_patterns = [
            r"å»è„‚ä½“é‡æŒ‡æ•°[\s=ï¼šä¸º]*(\d+\.?\d*)",
            r"ffmi[\s=ï¼šä¸º]*(\d+\.?\d*)",
            r"ç˜¦ä½“é‡æŒ‡æ•°[\s=ï¼šä¸º]*(\d+\.?\d*)"
        ]
        for pattern in ffmi_patterns:
            ffmi_match = re.search(pattern, data_lower)
            if ffmi_match:
                extracted_metrics["å»è„‚ä½“é‡æŒ‡æ•°"] = float(ffmi_match.group(1))
                break
        
        # ğŸ”¥ æ–°å¢ï¼šæå–ä½“æ€ç›¸å…³æŒ‡æ ‡
        posture_keywords = {
            "é«˜ä½è‚©": ["é«˜ä½è‚©", "è‚©è†€é«˜ä½", "è‚©é«˜ä½", "è‚©ä¸å¹³"],
            "å¤´å‰å€¾": ["å¤´å‰å€¾", "å¤´å‘å‰", "å¤´éƒ¨å‰å€¾", "é¢ˆå‰ä¼¸"],
            "å¤´ä¾§æ­ª": ["å¤´ä¾§æ­ª", "å¤´å€¾æ–œ", "å¤´æ­ª", "å¤´ä¾§å€¾", "å¤´å"],
            "å¤´å€¾æ–œ": ["å¤´å€¾æ–œ", "å¤´ä¾§æ­ª", "å¤´æ­ªæ–œ"],
            "éª¨ç›†å‰ç§»": ["éª¨ç›†å‰ç§»", "éª¨ç›†å‰å€¾", "ç›†éª¨å‰ç§»"],
            "åœ†è‚©": ["åœ†è‚©", "è‚©å†…æ‰£", "è‚©è†€å†…æ‰£", "å«èƒ¸"],
            "é©¼èƒŒ": ["é©¼èƒŒ", "å¼“èƒŒ", "èƒŒéƒ¨å¼¯æ›²"],
            "å·¦åœ†è‚©": ["å·¦åœ†è‚©", "å·¦è‚©å†…æ‰£"],
            "å³åœ†è‚©": ["å³åœ†è‚©", "å³è‚©å†…æ‰£"],
            "å·¦è…¿Xå‹": ["å·¦è…¿xå‹", "å·¦è…¿x", "å·¦è…¿å¤–ç¿»"],
            "å³è…¿Xå‹": ["å³è…¿xå‹", "å³è…¿x", "å³è…¿å¤–ç¿»"],
            "è…¿å‹å¼‚å¸¸": ["è…¿å‹", "è…¿å½¢", "xå‹è…¿", "oå‹è…¿"]
        }
        
        for main_term, variations in posture_keywords.items():
            for variation in variations:
                if variation in data_lower:
                    extracted_metrics[main_term] = "å­˜åœ¨"  # æ ‡è®°å­˜åœ¨è¯¥ä½“æ€é—®é¢˜
                    break
        
        # æ•°æ®åˆ†ç±»
        data_categories = []
        composition_metrics = [k for k in extracted_metrics.keys() if k in ["BMI", "ä½“è„‚ç‡", "å»è„‚ä½“é‡", "ä½“é‡", "èº«é«˜"]]
        posture_metrics = [k for k in extracted_metrics.keys() if k in posture_keywords.keys()]
        
        if composition_metrics:
            data_categories.append("ä½“æˆåˆ†æ•°æ®")
        if posture_metrics:
            data_categories.append("ä½“æ€æ•°æ®")
        if "æ€§åˆ«" in extracted_metrics:
            data_categories.append("åŸºæœ¬ä¿¡æ¯")
        
        result["body_data_analysis"]["extracted_metrics"] = extracted_metrics
        result["body_data_analysis"]["data_categories"] = data_categories
        result["body_data_analysis"]["analysis_summary"] = (
            f"æˆåŠŸè§£æå‡º{len(extracted_metrics)}é¡¹èº«ä½“æŒ‡æ ‡ï¼š{list(extracted_metrics.keys())}ã€‚"
            f"æ•°æ®ç±»åˆ«ï¼š{data_categories}"
        )
        
        return json.dumps(result, ensure_ascii=False, indent=2)
    
    @staticmethod
    def query_medical_knowledge(query_text, knowledge_base_name=None):
        """æŸ¥è¯¢èº«ä½“å¼‚å¸¸åˆ¤æ–­å†³ç­–æ ‘çŸ¥è¯†åº“"""
        DB_PATH = "VectorStore"
        
        try:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šçŸ¥è¯†åº“ï¼Œå°è¯•æ‰¾åˆ°ä¸€ä¸ªå¯ç”¨çš„
            if not knowledge_base_name or knowledge_base_name == "None":
                available_dbs = os.listdir(DB_PATH) if os.path.exists(DB_PATH) else []
                if available_dbs:
                    knowledge_base_name = available_dbs[0]
                else:
                    return json.dumps({"error": "æœªæ‰¾åˆ°å¯ç”¨çš„å†³ç­–æ ‘çŸ¥è¯†åº“"}, ensure_ascii=False)
            
            # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
            db_path = os.path.join(DB_PATH, knowledge_base_name)
            if not os.path.exists(db_path):
                return json.dumps({"error": f"å†³ç­–æ ‘çŸ¥è¯†åº“ {knowledge_base_name} ä¸å­˜åœ¨"}, ensure_ascii=False)
            
            # ä¼˜åŒ–æŸ¥è¯¢æ–‡æœ¬ - æå–å…³é”®èº«ä½“æŒ‡æ ‡è¯
            optimized_query = MedicalAnalysis._optimize_body_query(query_text)
            
            # é…ç½®é‡æ’åº
            dashscope_rerank = DashScopeRerank(
                top_n=10,  # å¢åŠ é‡æ’åºæ•°é‡ä»¥è·å–æ›´å¤šå†³ç­–è§„åˆ™
                return_documents=True,
                api_key="sk-51d30a5436ca433b8ff81e624a23dcac"
            )
            
            # åŠ è½½çŸ¥è¯†åº“
            storage_context = StorageContext.from_defaults(persist_dir=db_path)
            index = load_index_from_storage(storage_context)
            
            # æ£€ç´¢ç›¸å…³æ–‡æ¡£
            retriever_engine = index.as_retriever(similarity_top_k=25)
            retrieve_chunk = retriever_engine.retrieve(optimized_query)
            
            # é‡æ’åº
            try:
                results = dashscope_rerank.postprocess_nodes(retrieve_chunk, query_str=optimized_query)
                print(f"å†³ç­–æ ‘é‡æ’åºæˆåŠŸï¼Œè·å¾—{len(results)}ä¸ªç»“æœ")
            except Exception as e:
                print(f"å†³ç­–æ ‘é‡æ’åºå¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ")
                results = retrieve_chunk[:10]
            
            # æ•´ç†æ£€ç´¢ç»“æœ - ä¸“æ³¨äºå†³ç­–è§„åˆ™
            decision_rules = []
            for i, result in enumerate(results):
                score = result.score
                # å¯¹äºå†³ç­–æ ‘ï¼Œé™ä½é˜ˆå€¼ä»¥è·å–æ›´å¤šè§„åˆ™
                if score >= 0.05 or i < 5:
                    decision_rules.append({
                        "rule_id": i + 1,
                        "content": result.text[:600] + "..." if len(result.text) > 600 else result.text,
                        "confidence_score": round(score, 3),
                        "full_content": result.text
                    })
            
            return json.dumps({
                "query": optimized_query,
                "original_query": query_text,
                "knowledge_base": knowledge_base_name,
                "retrieved_chunks": decision_rules,
                "total_rules": len(decision_rules),
                "query_type": "èº«ä½“å¼‚å¸¸å†³ç­–æ ‘æŸ¥è¯¢"
            }, ensure_ascii=False, indent=2)
            
        except Exception as e:
            return json.dumps({
                "error": f"æŸ¥è¯¢å†³ç­–æ ‘çŸ¥è¯†åº“æ—¶å‡ºé”™ï¼š{str(e)}",
                "query": query_text
            }, ensure_ascii=False)
    
    @staticmethod
    def _optimize_body_query(query_text):
        """ä¼˜åŒ–æŸ¥è¯¢æ–‡æœ¬ï¼Œæå–å…³é”®èº«ä½“æŒ‡æ ‡è¯æ±‡ï¼ˆå¢å¼ºç‰ˆï¼Œæ”¯æŒä½“æ€æŒ‡æ ‡ï¼‰"""
        # ğŸ”¥ å¢å¼ºçš„èº«ä½“æŒ‡æ ‡å…³é”®è¯æ˜ å°„ï¼ŒåŒ…å«ä½“æˆåˆ†å’Œä½“æ€æŒ‡æ ‡
        body_keywords = {
            # ä½“æˆåˆ†ç›¸å…³æŒ‡æ ‡
            "BMI": ["bmi", "ä½“é‡æŒ‡æ•°", "ä½“è´¨æŒ‡æ•°"],
            "å»è„‚ä½“é‡æŒ‡æ•°": ["å»è„‚ä½“é‡æŒ‡æ•°", "ffmi", "ç˜¦ä½“é‡æŒ‡æ•°", "è‚Œè‚‰æŒ‡æ•°"],
            "ä½“é‡": ["ä½“é‡", "é‡é‡", "kg"],
            "èº«é«˜": ["èº«é«˜", "cm", "å˜ç±³"],
            "ä½“è„‚ç‡": ["ä½“è„‚ç‡", "ä½“è„‚"],
            
            # ä½“æ€ç›¸å…³æŒ‡æ ‡
            "é«˜ä½è‚©": ["é«˜ä½è‚©", "è‚©è†€é«˜ä½", "è‚©é«˜ä½", "è‚©ä¸å¹³"],
            "å¤´å‰å¼•": ["å¤´å‰å¼•", "å¤´å‘å‰", "å¤´éƒ¨å‰å€¾", "é¢ˆå‰ä¼¸"],
            "å¤´ä¾§æ­ª": ["å¤´ä¾§æ­ª", "å¤´æ­ª", "å¤´ä¾§å€¾", "å¤´å"],
            "éª¨ç›†å‰ç§»": ["éª¨ç›†å‰ç§»"],
            "éª¨ç›†æ—‹ç§»": ["éª¨ç›†æ—‹ç§»"],
            "åœ†è‚©": ["åœ†è‚©", "è‚©å†…æ‰£", "è‚©è†€å†…æ‰£"],
            "é©¼èƒŒ": ["é©¼èƒŒ", "å¼“èƒŒ", "èƒŒéƒ¨å¼¯æ›²"],
            "å·¦åœ†è‚©": ["å·¦åœ†è‚©", "å·¦è‚©å†…æ‰£"],
            "å³åœ†è‚©": ["å³åœ†è‚©", "å³è‚©å†…æ‰£"],
            "è…¿å‹": ["è…¿å‹", "è…¿å½¢", "xå‹è…¿", "oå‹è…¿", "å·¦è…¿", "å³è…¿"],
            "ä½“æ€": ["ä½“æ€", "å§¿åŠ¿", "ä½“å§¿", "èº«ä½“å§¿æ€"],
            
            # åŸºæœ¬ä¿¡æ¯
            "ç”·æ€§": ["ç”·æ€§", "ç”·", "male"],
            "å¥³æ€§": ["å¥³æ€§", "å¥³", "female"]
        }
        
        # æå–æŸ¥è¯¢æ–‡æœ¬ä¸­çš„å…³é”®æŒ‡æ ‡
        extracted_terms = []
        query_lower = query_text.lower()
        
        for main_term, variations in body_keywords.items():
            if any(var in query_lower for var in variations):
                extracted_terms.append(main_term)
        
        # æå–æ•°å€¼ä¿¡æ¯
        numbers = re.findall(r'\d+\.?\d*', query_text)
        
        # æ„å»ºä¼˜åŒ–æŸ¥è¯¢
        if extracted_terms:
            optimized = " ".join(extracted_terms) + " " + query_text
            if numbers:
                optimized += " " + " ".join(numbers)
        else:
            optimized = query_text
        
        print(f"åŸå§‹æŸ¥è¯¢: {query_text}")
        print(f"æå–çš„å…³é”®è¯: {extracted_terms}")
        print(f"ä¼˜åŒ–åæŸ¥è¯¢: {optimized}")
        
        return optimized

class HealthAssessment:
    """èº«ä½“å¼‚å¸¸è¯„ä¼°å·¥å…·ç±»"""
    
    @staticmethod
    def body_composition_analysis(user_data, decision_rules):
        """åŸºäºç”¨æˆ·ä½“æˆåˆ†æ•°æ®å’Œå†³ç­–æ ‘è§„åˆ™è¿›è¡Œä½“æˆåˆ†å¼‚å¸¸åˆ†æ"""
        return HealthAssessment._analyze_specific_type(user_data, decision_rules, "ä½“æˆåˆ†å¼‚å¸¸", 1)
    
    @staticmethod
    def posture_analysis(user_data, decision_rules):
        """åŸºäºç”¨æˆ·ä½“æ€æ•°æ®å’Œå†³ç­–æ ‘è§„åˆ™è¿›è¡Œä½“æ€å¼‚å¸¸åˆ†æï¼Œæœ€å¤šç”Ÿæˆ4ä¸ªå¼‚å¸¸"""
        return HealthAssessment._analyze_specific_type(user_data, decision_rules, "ä½“æ€å¼‚å¸¸", 4)
    
    @staticmethod
    def _analyze_specific_type(user_data, decision_rules, analysis_type, max_count):
        """é€šç”¨çš„å¼‚å¸¸åˆ†ææ–¹æ³•ï¼Œä¸¥æ ¼æŒ‰ç…§çŸ¥è¯†åº“ä¼˜å…ˆçº§æ’åº"""
        result = {
            f"{analysis_type}_assessment": {
                "analysis_type": analysis_type,
                "max_abnormalities": max_count,
                "user_data_input": user_data,
                "decision_rules_applied": decision_rules,
                "extracted_user_metrics": {},
                "parsed_knowledge_rules": [],
                "decision_process_steps": [],
                "abnormality_findings": [],
                "priority_sorted_results": [],
                "final_conclusions": [],
                "detailed_analysis": "",
                "disclaimer": f"æ­¤{analysis_type}åˆ†æç»“æœä¸¥æ ¼åŸºäºçŸ¥è¯†åº“å†³ç­–æ ‘è§„åˆ™ï¼Œä»…ä¾›å‚è€ƒï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šçš„å¥åº·é¡¾é—®æˆ–åŒ»ç”Ÿè·å–å‡†ç¡®è¯Šæ–­"
            }
        }
        
        try:
            # è§£æç”¨æˆ·æ•°æ®
            user_data_parsed = json.loads(user_data) if isinstance(user_data, str) else user_data
            extracted_metrics = {}
            
            if isinstance(user_data_parsed, dict) and "body_data_analysis" in user_data_parsed:
                all_metrics = user_data_parsed["body_data_analysis"].get("extracted_metrics", {})
                
                # æ ¹æ®åˆ†æç±»å‹ç­›é€‰ç›¸å…³æŒ‡æ ‡
                if analysis_type == "ä½“æˆåˆ†å¼‚å¸¸":
                    # ä½“æˆåˆ†ç›¸å…³æŒ‡æ ‡
                    composition_keys = ["BMI", "ä½“è„‚ç‡", "å»è„‚ä½“é‡", "ä½“é‡", "èº«é«˜", "å¹´é¾„", "æ€§åˆ«"]
                    extracted_metrics = {k: v for k, v in all_metrics.items() if k in composition_keys}
                elif analysis_type == "ä½“æ€å¼‚å¸¸":
                    # ä½“æ€ç›¸å…³æŒ‡æ ‡
                    posture_keys = ["é«˜ä½è‚©", "å¤´å‰å€¾", "å¤´å€¾æ–œ", "å·¦åœ†è‚©", "å³åœ†è‚©", "å·¦è…¿Xå‹", "å³è…¿Xå‹"]
                    extracted_metrics = {k: v for k, v in all_metrics.items() if k in posture_keys or "å›´åº¦" in k}
                    # ä¹ŸåŒ…å«åŸºæœ¬ä¿¡æ¯ç”¨äºä½“æ€åˆ†æ
                    basic_keys = ["èº«é«˜", "å¹´é¾„", "æ€§åˆ«"]
                    for key in basic_keys:
                        if key in all_metrics:
                            extracted_metrics[key] = all_metrics[key]
            
            result[f"{analysis_type}_assessment"]["extracted_user_metrics"] = extracted_metrics
            
            # è§£æå†³ç­–è§„åˆ™
            decision_data = json.loads(decision_rules) if isinstance(decision_rules, str) else decision_rules
            knowledge_rules = []
            
            if isinstance(decision_data, dict) and "retrieved_chunks" in decision_data:
                knowledge_rules = decision_data["retrieved_chunks"]
            
            # è§£æçŸ¥è¯†åº“ä¸­çš„ç»“æ„åŒ–å†³ç­–è§„åˆ™
            parsed_rules = []
            for rule in knowledge_rules:
                content = rule.get("full_content", rule.get("content", ""))
                parsed_rule = HealthAssessment._parse_decision_rule(content)
                
                if parsed_rule:
                    # å®Œå–„è§„åˆ™ä¿¡æ¯
                    parsed_rule["confidence_score"] = rule.get("confidence_score", 0.5)
                    parsed_rule["rule_id"] = rule.get("rule_id", f"rule_{len(parsed_rules)+1}")
                    parsed_rule["original_content"] = content
                    
                    # æ ¹æ®åˆ†æç±»å‹ç­›é€‰è§„åˆ™
                    rule_content_lower = content.lower()
                    is_relevant = False
                    
                    if analysis_type == "ä½“æˆåˆ†å¼‚å¸¸":
                        if any(word in rule_content_lower for word in ["bmi", "ä½“è„‚", "ä½“é‡", "è‚¥èƒ–", "æ¶ˆç˜¦", "ä½“æˆåˆ†"]):
                            is_relevant = True
                    elif analysis_type == "ä½“æ€å¼‚å¸¸":
                        if any(word in rule_content_lower for word in ["é«˜ä½è‚©", "å¤´å‰å€¾", "åœ†è‚©", "ä½“æ€", "å§¿åŠ¿", "è…¿å‹", "å¤´å€¾æ–œ"]):
                            is_relevant = True
                    
                    if is_relevant:
                        parsed_rules.append(parsed_rule)
            
            # ä¸¥æ ¼æŒ‰ç…§çŸ¥è¯†åº“ä¼˜å…ˆçº§æ’åºï¼ˆä¼˜å…ˆçº§æ•°å­—è¶Šå°è¶Šé‡è¦ï¼‰
            def get_sort_key(rule):
                priority = rule.get("priority", 999)  # é»˜è®¤æœ€ä½ä¼˜å…ˆçº§
                confidence = rule.get("confidence_score", 0.0)
                return (priority, -confidence)  # å…ˆæŒ‰ä¼˜å…ˆçº§å‡åºï¼Œå†æŒ‰ç½®ä¿¡åº¦é™åº
            
            parsed_rules.sort(key=get_sort_key)
            result[f"{analysis_type}_assessment"]["parsed_knowledge_rules"] = parsed_rules
            
            # åº”ç”¨å†³ç­–æ ‘è§„åˆ™è¿›è¡Œåˆ¤æ–­
            matched_rules = []
            decision_steps = []
            abnormality_findings = []
            priority_results = []
            
            # é™åˆ¶å¼‚å¸¸å‘ç°çš„æ•°é‡
            abnormality_count = 0
            
            for rule in parsed_rules:
                if abnormality_count >= max_count:
                    break
                
                # æ£€æŸ¥è§„åˆ™æ˜¯å¦é€‚ç”¨äºå½“å‰ç”¨æˆ·æ•°æ®
                is_applicable, match_result = HealthAssessment._check_rule_applicability(extracted_metrics, rule)
                
                # è®°å½•å†³ç­–æ­¥éª¤ï¼ˆæ— è®ºæ˜¯å¦åŒ¹é…ï¼‰
                decision_step = {
                    "step": len(decision_steps) + 1,
                    "rule_id": rule["rule_id"],
                    "priority": rule.get("priority", 999),
                    "condition": rule.get("condition", ""),
                    "user_data_values": match_result,
                    "is_match": is_applicable,
                    "conclusion": rule.get("conclusion", "") if is_applicable else "æ¡ä»¶ä¸åŒ¹é…ï¼Œè·³è¿‡æ­¤è§„åˆ™",
                    "confidence_score": rule["confidence_score"],
                    "detailed_process": ""
                }
                
                if is_applicable:
                    # æ„å»ºè¯¦ç»†çš„åˆ¤æ–­è¿‡ç¨‹æè¿°
                    process_detail = f"ã€å†³ç­–æµç¨‹{len(decision_steps) + 1}ã€‘\n"
                    process_detail += f"è§„åˆ™ID: {rule['rule_id']} (ä¼˜å…ˆçº§: {rule.get('priority', 'æœªè®¾ç½®')})\n"
                    process_detail += f"åˆ¤æ–­æ¡ä»¶: {rule.get('condition', '')}\n"
                    process_detail += f"ç”¨æˆ·æ•°æ®åŒ¹é…æƒ…å†µ:\n"
                    
                    for condition_key, user_value in match_result.items():
                        process_detail += f"  - {condition_key}: ç”¨æˆ·å€¼={user_value} âœ“åŒ¹é…\n"
                    
                    process_detail += f"åŒ¹é…ç»“æœ: æ»¡è¶³æ¡ä»¶\n"
                    process_detail += f"å¼‚å¸¸ç»“è®º: {rule.get('conclusion', '')}\n"
                    
                    if rule.get("solution"):
                        process_detail += f"è§£å†³æ–¹æ¡ˆ: {rule['solution']}\n"
                    
                    process_detail += f"ç½®ä¿¡åº¦: {rule['confidence_score']:.2f}\n"
                    
                    decision_step["detailed_process"] = process_detail
                    matched_rules.append(rule)
                    abnormality_count += 1
                    
                    # æ·»åŠ å¼‚å¸¸å‘ç°
                    if rule.get("conclusion"):
                        finding = {
                            "priority": rule.get("priority", 999),
                            "conclusion": rule["conclusion"],
                            "condition_matched": rule.get("condition", ""),
                            "user_values": match_result,
                            "confidence": rule["confidence_score"],
                            "solution": rule.get("solution", ""),
                            "rule_id": rule["rule_id"]
                        }
                        abnormality_findings.append(finding)
                        priority_results.append(finding)
                else:
                    # ä¸åŒ¹é…çš„æƒ…å†µ
                    decision_step["detailed_process"] = f"ã€å†³ç­–æµç¨‹{len(decision_steps) + 1}ã€‘\n"
                    decision_step["detailed_process"] += f"è§„åˆ™ID: {rule['rule_id']}\n"
                    decision_step["detailed_process"] += f"åˆ¤æ–­æ¡ä»¶: {rule.get('condition', '')}\n"
                    decision_step["detailed_process"] += f"ç”¨æˆ·æ•°æ®: {extracted_metrics}\n"
                    decision_step["detailed_process"] += f"åŒ¹é…ç»“æœ: ä¸æ»¡è¶³æ¡ä»¶ï¼Œè·³è¿‡\n"
                
                decision_steps.append(decision_step)
            
            # æŒ‰ä¼˜å…ˆçº§æ’åºæœ€ç»ˆç»“æœ
            priority_results.sort(key=lambda x: (x["priority"], -x["confidence"]))
            
            # ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š
            detailed_analysis = f"\n=== {analysis_type}è¯¦ç»†åˆ†ææŠ¥å‘Š ===\n\n"
            detailed_analysis += f"ç”¨æˆ·å…³é”®æŒ‡æ ‡: {extracted_metrics}\n\n"
            detailed_analysis += f"çŸ¥è¯†åº“è§„åˆ™æ€»æ•°: {len(parsed_rules)}\n"
            detailed_analysis += f"åŒ¹é…è§„åˆ™æ•°é‡: {len(matched_rules)}\n"
            detailed_analysis += f"å‘ç°å¼‚å¸¸æ•°é‡: {len(abnormality_findings)}\n\n"
            
            if matched_rules:
                detailed_analysis += "=== æŒ‰ä¼˜å…ˆçº§æ’åºçš„å¼‚å¸¸åˆ†æ ===\n\n"
                for i, finding in enumerate(priority_results, 1):
                    detailed_analysis += f"{i}. ã€ä¼˜å…ˆçº§ {finding['priority']}ã€‘{finding['conclusion']}\n"
                    detailed_analysis += f"   åˆ¤æ–­ä¾æ®: {finding['condition_matched']}\n"
                    detailed_analysis += f"   æ•°æ®åŒ¹é…: {finding['user_values']}\n"
                    detailed_analysis += f"   ç½®ä¿¡åº¦: {finding['confidence']:.2f}\n"
                    if finding['solution']:
                        detailed_analysis += f"   å»ºè®®æªæ–½: {finding['solution']}\n"
                    detailed_analysis += "\n"
                
                detailed_analysis += "=== å®Œæ•´å†³ç­–æµç¨‹ ===\n\n"
                for step in decision_steps:
                    if step["is_match"]:
                        detailed_analysis += step["detailed_process"] + "\n"
            else:
                detailed_analysis += "=== åˆ†æç»“æœ ===\n\n"
                detailed_analysis += f"ç»è¿‡{len(parsed_rules)}æ¡å†³ç­–è§„åˆ™çš„ä¸¥æ ¼åˆ¤æ–­ï¼Œç”¨æˆ·çš„{analysis_type}æ•°æ®æœªåŒ¹é…åˆ°ä»»ä½•å¼‚å¸¸æ¡ä»¶ã€‚\n"
                detailed_analysis += "å½“å‰å„é¡¹æŒ‡æ ‡åœ¨çŸ¥è¯†åº“å®šä¹‰çš„æ­£å¸¸èŒƒå›´å†…ã€‚\n\n"
                
                if parsed_rules:
                    detailed_analysis += "=== æ£€æŸ¥çš„å†³ç­–è§„åˆ™ ===\n\n"
                    for i, rule in enumerate(parsed_rules[:5], 1):  # æ˜¾ç¤ºå‰5æ¡è§„åˆ™
                        detailed_analysis += f"{i}. è§„åˆ™ID: {rule['rule_id']}\n"
                        detailed_analysis += f"   æ¡ä»¶: {rule.get('condition', '')}\n"
                        detailed_analysis += f"   ç»“è®º: {rule.get('conclusion', '')}\n"
                        detailed_analysis += f"   ç”¨æˆ·æ•°æ®ä¸åŒ¹é…æ­¤æ¡ä»¶\n\n"
            print("detailed_analysis",detailed_analysis)
            
            # ç”Ÿæˆæœ€ç»ˆç»“è®º
            final_conclusions = []
            if priority_results:
                final_conclusions.append(f"åŸºäºçŸ¥è¯†åº“å†³ç­–æ ‘è§„åˆ™ï¼Œå‘ç°{len(priority_results)}é¡¹{analysis_type}ï¼š")
                for finding in priority_results:
                    final_conclusions.append(f"- {finding['conclusion']} (ä¼˜å…ˆçº§{finding['priority']})")
            else:
                final_conclusions.append(f"åŸºäºçŸ¥è¯†åº“å†³ç­–æ ‘è§„åˆ™åˆ†æï¼Œæœªå‘ç°{analysis_type}ã€‚")
                final_conclusions.append("ç”¨æˆ·ç›¸å…³æŒ‡æ ‡å‡åœ¨æ­£å¸¸èŒƒå›´å†…ã€‚")
            
            result[f"{analysis_type}_assessment"]["decision_process_steps"] = decision_steps
            result[f"{analysis_type}_assessment"]["abnormality_findings"] = abnormality_findings
            result[f"{analysis_type}_assessment"]["priority_sorted_results"] = priority_results
            result[f"{analysis_type}_assessment"]["final_conclusions"] = final_conclusions
            result[f"{analysis_type}_assessment"]["detailed_analysis"] = detailed_analysis
            
        except Exception as e:
            error_msg = f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}"
            result[f"{analysis_type}_assessment"]["error"] = error_msg
            result[f"{analysis_type}_assessment"]["final_conclusions"] = [f"{analysis_type}åˆ†æå¤±è´¥: {error_msg}"]
            result[f"{analysis_type}_assessment"]["detailed_analysis"] = f"é”™è¯¯è¯¦æƒ…: {error_msg}"
        
        return json.dumps(result, ensure_ascii=False, indent=2)
    
    @staticmethod
    def comprehensive_analysis(user_data, decision_rules):
        """åŸºäºç”¨æˆ·èº«ä½“æ•°æ®å’Œå†³ç­–æ ‘è§„åˆ™è¿›è¡Œç»¼åˆå¼‚å¸¸åˆ†æ"""
        result = {
            "abnormality_assessment": {
                "user_data_input": user_data,
                "decision_rules_applied": decision_rules,
                "extracted_user_metrics": {},
                "applied_decision_rules": [],
                "decision_process": [],
                "abnormality_findings": [],
                "risk_level": "æœªçŸ¥",
                "specific_conclusions": [],
                "recommendations": [],
                "disclaimer": "æ­¤åˆ†æç»“æœä¸¥æ ¼åŸºäºçŸ¥è¯†åº“å†³ç­–æ ‘è§„åˆ™ï¼Œä»…ä¾›å‚è€ƒï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šçš„å¥åº·é¡¾é—®æˆ–åŒ»ç”Ÿè·å–å‡†ç¡®è¯Šæ–­"
            }
        }
        
        try:
            # è§£æç”¨æˆ·æ•°æ®
            user_data_parsed = json.loads(user_data) if isinstance(user_data, str) else user_data
            extracted_metrics = {}
            
            if isinstance(user_data_parsed, dict) and "body_data_analysis" in user_data_parsed:
                extracted_metrics = user_data_parsed["body_data_analysis"].get("extracted_metrics", {})
            
            result["abnormality_assessment"]["extracted_user_metrics"] = extracted_metrics
            
            # è§£æå†³ç­–è§„åˆ™
            decision_data = json.loads(decision_rules) if isinstance(decision_rules, str) else decision_rules
            knowledge_rules = []
            
            if isinstance(decision_data, dict) and "retrieved_chunks" in decision_data:
                knowledge_rules = decision_data["retrieved_chunks"]
            
            # è§£æçŸ¥è¯†åº“ä¸­çš„ç»“æ„åŒ–å†³ç­–è§„åˆ™
            parsed_rules = []
            for rule in knowledge_rules:
                content = rule.get("full_content", rule.get("content", ""))
                parsed_rule = HealthAssessment._parse_decision_rule(content)
                if parsed_rule:
                    parsed_rule["confidence_score"] = rule.get("confidence_score", 0.5)
                    parsed_rule["rule_id"] = rule.get("rule_id", "unknown")
                    parsed_rules.append(parsed_rule)
            
            # æŒ‰ç½®ä¿¡åº¦æ’åºè§„åˆ™
            parsed_rules.sort(key=lambda x: x["confidence_score"], reverse=True)
            
            # åº”ç”¨å†³ç­–æ ‘è§„åˆ™è¿›è¡Œåˆ¤æ–­
            applied_rules = []
            decision_process = []
            abnormality_findings = []
            specific_conclusions = []
            recommendations = []
            
            for rule in parsed_rules:
                # æ£€æŸ¥è§„åˆ™æ˜¯å¦é€‚ç”¨äºå½“å‰ç”¨æˆ·æ•°æ®
                is_applicable, match_result = HealthAssessment._check_rule_applicability(extracted_metrics, rule)
                
                if is_applicable:
                    applied_rules.append(rule)
                    
                    # è®°å½•å†³ç­–è¿‡ç¨‹
                    process_step = {
                        "rule_id": rule["rule_id"],
                        "condition": rule.get("condition", ""),
                        "user_values": match_result,
                        "result": rule.get("conclusion", ""),
                        "confidence": rule["confidence_score"]
                    }
                    decision_process.append(process_step)
                    
                    # æ·»åŠ å¼‚å¸¸å‘ç°
                    if rule.get("conclusion"):
                        abnormality_findings.append(rule["conclusion"])
                        specific_conclusions.append(
                            f"æ ¹æ®å†³ç­–è§„åˆ™ï¼š{rule.get('condition', '')} -> ç»“è®ºï¼š{rule['conclusion']} "
                            f"(ç”¨æˆ·æ•°æ®åŒ¹é…ï¼š{match_result})"
                        )
                    
                    # æ·»åŠ å»ºè®®
                    if rule.get("solution"):
                        recommendations.append(f"é’ˆå¯¹{rule['conclusion']}ï¼š{rule['solution']}")
                    
                    if rule.get("recommendation"):
                        recommendations.append(rule["recommendation"])
            
            # å¦‚æœæ²¡æœ‰åŒ¹é…çš„è§„åˆ™ï¼Œè¡¨ç¤ºçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³çš„å†³ç­–è§„åˆ™
            if not applied_rules:
                decision_process.append({
                    "rule_id": "æ— åŒ¹é…è§„åˆ™",
                    "condition": "æœªåœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°é€‚ç”¨çš„å†³ç­–è§„åˆ™",
                    "user_values": extracted_metrics,
                    "result": "æ— æ³•åŸºäºçŸ¥è¯†åº“è§„åˆ™è¿›è¡Œåˆ¤æ–­",
                    "confidence": 0.0
                })
                
                specific_conclusions.append("çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°é€‚ç”¨äºå½“å‰ç”¨æˆ·æ•°æ®çš„å†³ç­–è§„åˆ™ï¼Œæ— æ³•è¿›è¡Œå¼‚å¸¸åˆ†æ")
                risk_level = "æ— æ³•è¯„ä¼°"
            else:
                # å®Œå…¨åŸºäºçŸ¥è¯†åº“è§„åˆ™çš„åˆ¤æ–­ç»“æœ
                # æ ¹æ®çŸ¥è¯†åº“ä¸­è§„åˆ™çš„ä¼˜å…ˆçº§æ¥ç¡®å®šé£é™©ç­‰çº§
                if applied_rules:
                    # è·å–æœ€é«˜ä¼˜å…ˆçº§ï¼ˆæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
                    priorities = [rule.get("priority", 5) for rule in applied_rules if rule.get("priority")]
                    if priorities:
                        highest_priority = min(priorities)
                        if highest_priority == 1:
                            risk_level = "é«˜é£é™©"
                        elif highest_priority == 2:
                            risk_level = "ä¸­é«˜é£é™©"
                        elif highest_priority == 3:
                            risk_level = "ä¸­é£é™©"
                        else:
                            risk_level = "ä½é£é™©"
                    else:
                        # å¦‚æœæ²¡æœ‰ä¼˜å…ˆçº§ä¿¡æ¯ï¼Œæ ¹æ®å¼‚å¸¸å‘ç°æ•°é‡åˆ¤æ–­
                        if len(abnormality_findings) >= 2:
                            risk_level = "é«˜é£é™©"
                        elif len(abnormality_findings) == 1:
                            risk_level = "ä¸­é£é™©"
                        else:
                            risk_level = "ä½é£é™©"
                else:
                    risk_level = "ä½é£é™©"
                
                # å¦‚æœæ²¡æœ‰å¼‚å¸¸å‘ç°ï¼Œè¡¨ç¤ºæŒ‡æ ‡æ­£å¸¸
                if not abnormality_findings:
                    specific_conclusions.append("åŸºäºçŸ¥è¯†åº“å†³ç­–è§„åˆ™åˆ†æï¼Œå½“å‰ç”¨æˆ·æ•°æ®æœªåŒ¹é…åˆ°ä»»ä½•å¼‚å¸¸æ¡ä»¶ï¼Œå„é¡¹æŒ‡æ ‡åœ¨æ­£å¸¸èŒƒå›´å†…")
                    risk_level = "ä½é£é™©"
            
            result["abnormality_assessment"]["applied_decision_rules"] = applied_rules
            result["abnormality_assessment"]["decision_process"] = decision_process
            result["abnormality_assessment"]["abnormality_findings"] = abnormality_findings
            result["abnormality_assessment"]["risk_level"] = risk_level
            result["abnormality_assessment"]["specific_conclusions"] = specific_conclusions
            result["abnormality_assessment"]["recommendations"] = list(set(recommendations))
            
        except Exception as e:
            result["abnormality_assessment"]["error"] = f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}"
            result["abnormality_assessment"]["specific_conclusions"] = ["åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ•°æ®æ ¼å¼"]
        
        return json.dumps(result, ensure_ascii=False, indent=2)
    
    @staticmethod
    def _parse_decision_rule(rule_content):
        """è§£æçŸ¥è¯†åº“ä¸­çš„å†³ç­–è§„åˆ™ï¼Œæå–å¼‚å¸¸ç»“è®ºã€åˆ¤æ–­æµç¨‹å’Œä¼˜å…ˆçº§"""
        try:
            # è§£æç»“æ„åŒ–çš„å†³ç­–è§„åˆ™
            parsed = {}
            
            # æå–å¼‚å¸¸ç»“è®º - æ”¯æŒå¤šç§æ ¼å¼
            conclusion_patterns = [
                r"å¼‚å¸¸ç»“è®º[:ï¼š]\s*([^\n,ï¼Œ]+)",
                r"ç»“è®º[:ï¼š]\s*([^\n,ï¼Œ]+)",
                r"è¯Šæ–­[:ï¼š]\s*([^\n,ï¼Œ]+)",
                r"å¼‚å¸¸[:ï¼š]\s*([^\n,ï¼Œ]+)"
            ]
            
            for pattern in conclusion_patterns:
                conclusion_match = re.search(pattern, rule_content)
                if conclusion_match:
                    parsed["conclusion"] = conclusion_match.group(1).strip()
                    break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¼‚å¸¸ç»“è®ºï¼Œå°è¯•ä»å†…å®¹ä¸­æ¨æ–­
            if "conclusion" not in parsed:
                # æŸ¥æ‰¾å¯èƒ½çš„å¼‚å¸¸è¯æ±‡
                abnormal_keywords = ["è‚¥èƒ–", "æ¶ˆç˜¦", "è¿‡é‡", "åç˜¦", "é«˜ä½è‚©", "å¤´å‰å€¾", "åœ†è‚©", "Xå‹è…¿", "Oå‹è…¿"]
                for keyword in abnormal_keywords:
                    if keyword in rule_content:
                        parsed["conclusion"] = keyword
                        break
            
            # æå–åˆ¤æ–­æµç¨‹/æ¡ä»¶ - æ”¯æŒå¤šç§æ ¼å¼
            condition_patterns = [
                r"å¼‚å¸¸åˆ¤æ–­æµç¨‹[:ï¼š]\s*([^\n]+)",
                r"åˆ¤æ–­æµç¨‹[:ï¼š]\s*([^\n]+)",
                r"åˆ¤æ–­æ¡ä»¶[^ï¼š:]*[:ï¼š]\s*([^\n]+)",
                r"æ¡ä»¶[:ï¼š]\s*([^\n]+)",
                r"å¦‚æœ[:ï¼š]\s*([^\n]+)",
                r"å½“[:ï¼š]\s*([^\n]+)",
                # BMIç›¸å…³æ¡ä»¶
                r"BMI\s*[<>â‰¤â‰¥]\s*[\d.]+[^\n]*",
                # ä½“è„‚ç‡ç›¸å…³æ¡ä»¶  
                r"ä½“è„‚ç‡?\s*[<>â‰¤â‰¥]\s*[\d.]+[%ï¼…]?[^\n]*",
                # å»è„‚ä½“é‡æŒ‡æ•°ç›¸å…³æ¡ä»¶
                r"å»è„‚ä½“é‡æŒ‡æ•°\s*[<>â‰¤â‰¥]\s*[\d.]+[^\n]*",
                r"[ç”·å¥³]æ€§å»è„‚ä½“é‡æŒ‡æ•°\s*[<>â‰¤â‰¥]\s*[\d.]+[^\n]*"
            ]
            
            for pattern in condition_patterns:
                condition_match = re.search(pattern, rule_content)
                if condition_match:
                    if ":" in pattern or "ï¼š" in pattern:
                        parsed["condition"] = condition_match.group(1).strip()
                    else:
                        parsed["condition"] = condition_match.group(0).strip()
                    break
            
            # æå–å…³é”®è§£å†³ç‚¹/è§£å†³æ–¹æ¡ˆ
            solution_patterns = [
                r"å…³é”®è§£å†³ç‚¹[^ï¼š:]*[:ï¼š]\s*([^\n,ï¼Œ]+)",
                r"è§£å†³æ–¹æ¡ˆ[^ï¼š:]*[:ï¼š]\s*([^\n,ï¼Œ]+)",
                r"è§£å†³[^ï¼š:]*[:ï¼š]\s*([^\n,ï¼Œ]+)",
                r"æ²»ç–—[^ï¼š:]*[:ï¼š]\s*([^\n,ï¼Œ]+)"
            ]
            
            for pattern in solution_patterns:
                solution_match = re.search(pattern, rule_content)
                if solution_match:
                    parsed["solution"] = solution_match.group(1).strip()
                    break
            
            # æå–å»ºè®®
            recommendation_patterns = [
                r"å»ºè®®[^ï¼š:]*[:ï¼š]\s*([^\n,ï¼Œ]+)",
                r"æ¨è[^ï¼š:]*[:ï¼š]\s*([^\n,ï¼Œ]+)",
                r"æ³¨æ„[^ï¼š:]*[:ï¼š]\s*([^\n,ï¼Œ]+)"
            ]
            
            for pattern in recommendation_patterns:
                recommendation_match = re.search(pattern, rule_content)
                if recommendation_match:
                    parsed["recommendation"] = recommendation_match.group(1).strip()
                    break
            
            # æå–ä¼˜å…ˆçº§ - æ”¯æŒå¤šç§æ ¼å¼
            priority_patterns = [
                r"ä¼˜å…ˆçº§[:ï¼š]\s*(\d+)",
                r"çº§åˆ«[:ï¼š]\s*(\d+)",
                r"ç­‰çº§[:ï¼š]\s*(\d+)",
                r"priority[:ï¼š]\s*(\d+)",
                r"level[:ï¼š]\s*(\d+)"
            ]
            
            for pattern in priority_patterns:
                priority_match = re.search(pattern, rule_content, re.IGNORECASE)
                if priority_match:
                    parsed["priority"] = int(priority_match.group(1))
                    break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¼˜å…ˆçº§ï¼Œå°è¯•ä»å…³é”®è¯æ¨æ–­
            if "priority" not in parsed:
                high_priority_keywords = ["ä¸¥é‡", "å±é™©", "æ€¥éœ€", "ç«‹å³", "ç´§æ€¥"]
                medium_priority_keywords = ["æ³¨æ„", "å»ºè®®", "éœ€è¦", "åº”è¯¥"]
                
                content_lower = rule_content.lower()
                for keyword in high_priority_keywords:
                    if keyword in content_lower:
                        parsed["priority"] = 1  # é«˜ä¼˜å…ˆçº§
                        break
                else:
                    for keyword in medium_priority_keywords:
                        if keyword in content_lower:
                            parsed["priority"] = 3  # ä¸­ä¼˜å…ˆçº§
                            break
                    else:
                        parsed["priority"] = 5  # é»˜è®¤ä½ä¼˜å…ˆçº§
            
            # æå–æ•°å€¼é˜ˆå€¼ï¼ˆç”¨äºåç»­æ¡ä»¶åŒ¹é…ï¼‰
            thresholds = {}
            
            # BMIé˜ˆå€¼
            bmi_matches = re.findall(r"BMI\s*([<>â‰¤â‰¥])\s*([\d.]+)", rule_content)
            for operator, value in bmi_matches:
                thresholds[f"BMI_{operator}"] = float(value)
            
            # ä½“è„‚ç‡é˜ˆå€¼
            body_fat_matches = re.findall(r"ä½“è„‚ç‡?\s*([<>â‰¤â‰¥])\s*([\d.]+)", rule_content)
            for operator, value in body_fat_matches:
                thresholds[f"ä½“è„‚ç‡_{operator}"] = float(value)
            
            # å»è„‚ä½“é‡æŒ‡æ•°é˜ˆå€¼
            ffmi_matches = re.findall(r"å»è„‚ä½“é‡æŒ‡æ•°\s*([<>â‰¤â‰¥])\s*([\d.]+)", rule_content)
            for operator, value in ffmi_matches:
                thresholds[f"å»è„‚ä½“é‡æŒ‡æ•°_{operator}"] = float(value)
            
            if thresholds:
                parsed["thresholds"] = thresholds
            
            # æå–é€‚ç”¨æ€§åˆ«
            if "ç”·æ€§" in rule_content:
                parsed["applicable_gender"] = "ç”·æ€§"
            elif "å¥³æ€§" in rule_content:
                parsed["applicable_gender"] = "å¥³æ€§"
            
            # è®°å½•åŸå§‹å†…å®¹ç”¨äºè°ƒè¯•
            parsed["raw_content"] = rule_content
            
            print(f"è§£æå†³ç­–è§„åˆ™ç»“æœ: {parsed}")  # è°ƒè¯•ä¿¡æ¯
            
            # åªæœ‰åœ¨è‡³å°‘æœ‰ç»“è®ºæˆ–æ¡ä»¶æ—¶æ‰è¿”å›è§£æç»“æœ
            if "conclusion" in parsed or "condition" in parsed:
                return parsed
            else:
                print(f"æœªèƒ½è§£æå‡ºæœ‰æ•ˆè§„åˆ™ï¼ŒåŸå§‹å†…å®¹ï¼š{rule_content[:100]}...")
                return None
            
        except Exception as e:
            print(f"è§£æå†³ç­–è§„åˆ™å¤±è´¥: {e}, å†…å®¹: {rule_content[:100]}...")
            return None
    
    @staticmethod
    def _check_rule_applicability(user_metrics, rule):
        """æ£€æŸ¥å†³ç­–è§„åˆ™æ˜¯å¦é€‚ç”¨äºç”¨æˆ·æ•°æ®ï¼Œæ”¯æŒä½“æˆåˆ†å’Œä½“æ€æŒ‡æ ‡"""
        try:
            condition = rule.get("condition", "")
            if not condition: 
                return False, {}
            
            # è·å–ç”¨æˆ·æŒ‡æ ‡
            user_bmi = user_metrics.get("BMI")
            user_body_fat = user_metrics.get("ä½“è„‚ç‡")
            user_ffmi = user_metrics.get("å»è„‚ä½“é‡æŒ‡æ•°", user_metrics.get("å»è„‚ä½“é‡"))
            user_weight = user_metrics.get("ä½“é‡")
            user_height = user_metrics.get("èº«é«˜")
            user_gender = user_metrics.get("æ€§åˆ«", "")
            user_age = user_metrics.get("å¹´é¾„")
            
            # ä½“æ€ç›¸å…³æŒ‡æ ‡
            user_shoulder_imbalance = user_metrics.get("é«˜ä½è‚©", 0)
            user_head_forward = user_metrics.get("å¤´å‰å€¾", 0)
            user_head_tilt = user_metrics.get("å¤´å€¾æ–œ", 0)
            user_left_round_shoulder = user_metrics.get("å·¦åœ†è‚©", 0)
            user_right_round_shoulder = user_metrics.get("å³åœ†è‚©", 0)
            user_left_leg_xo = user_metrics.get("å·¦è…¿Xå‹", 0)
            user_right_leg_xo = user_metrics.get("å³è…¿Xå‹", 0)
            
            match_result = {}
            all_conditions_met = True
            
            # æ£€æŸ¥æ€§åˆ«é€‚ç”¨æ€§
            applicable_gender = rule.get("applicable_gender")
            if applicable_gender and user_gender:
                if applicable_gender not in user_gender:
                    return False, {"gender_mismatch": f"è§„åˆ™é€‚ç”¨äº{applicable_gender}ï¼Œç”¨æˆ·ä¸º{user_gender}"}
            
            # æ£€æŸ¥BMIæ¡ä»¶
            bmi_conditions = re.findall(r"BMI\s*([<>â‰¤â‰¥])\s*([\d.]+)", condition)
            for operator, threshold in bmi_conditions:
                if user_bmi is not None:
                    threshold = float(threshold)
                    match_result[f"BMI{operator}{threshold}"] = user_bmi
                    
                    condition_met = False
                    if operator in ["<", "ï¼œ"]:
                        condition_met = user_bmi < threshold
                    elif operator in [">", "ï¼"]:
                        condition_met = user_bmi > threshold
                    elif operator in ["â‰¤", "<=", "â‰¦"]:
                        condition_met = user_bmi <= threshold
                    elif operator in ["â‰¥", ">=", "â‰§"]:
                        condition_met = user_bmi >= threshold
                    
                    if not condition_met:
                        all_conditions_met = False
                        match_result[f"BMI{operator}{threshold}_result"] = "ä¸æ»¡è¶³"
                    else:
                        match_result[f"BMI{operator}{threshold}_result"] = "æ»¡è¶³"
                else:
                    all_conditions_met = False
                    match_result[f"BMI{operator}{threshold}"] = "ç”¨æˆ·æ— BMIæ•°æ®"
            
            # æ£€æŸ¥ä½“è„‚ç‡æ¡ä»¶
            body_fat_conditions = re.findall(r"ä½“è„‚ç‡?\s*([<>â‰¤â‰¥])\s*([\d.]+)", condition)
            for operator, threshold in body_fat_conditions:
                if user_body_fat is not None:
                    threshold = float(threshold)
                    match_result[f"ä½“è„‚ç‡{operator}{threshold}"] = user_body_fat
                    
                    condition_met = False
                    if operator in ["<", "ï¼œ"]:
                        condition_met = user_body_fat < threshold
                    elif operator in [">", "ï¼"]:
                        condition_met = user_body_fat > threshold
                    elif operator in ["â‰¤", "<=", "â‰¦"]:
                        condition_met = user_body_fat <= threshold
                    elif operator in ["â‰¥", ">=", "â‰§"]:
                        condition_met = user_body_fat >= threshold
                    
                    if not condition_met:
                        all_conditions_met = False
                        match_result[f"ä½“è„‚ç‡{operator}{threshold}_result"] = "ä¸æ»¡è¶³"
                    else:
                        match_result[f"ä½“è„‚ç‡{operator}{threshold}_result"] = "æ»¡è¶³"
                else:
                    all_conditions_met = False
                    match_result[f"ä½“è„‚ç‡{operator}{threshold}"] = "ç”¨æˆ·æ— ä½“è„‚ç‡æ•°æ®"
            
            # æ£€æŸ¥å»è„‚ä½“é‡æŒ‡æ•°æ¡ä»¶
            ffmi_patterns = [
                r"([ç”·å¥³]æ€§)?å»è„‚ä½“é‡æŒ‡æ•°\s*([<>â‰¤â‰¥])\s*([\d.]+)",
                r"å»è„‚ä½“é‡[^æŒ‡æ•°]*([<>â‰¤â‰¥])\s*([\d.]+)"
            ]
            
            for pattern in ffmi_patterns:
                ffmi_matches = re.findall(pattern, condition)
                for match in ffmi_matches:
                    if len(match) == 3:  # åŒ…å«æ€§åˆ«çš„åŒ¹é…
                        gender, operator, threshold = match
                        if gender and user_gender and gender in user_gender and user_ffmi is not None:
                            threshold = float(threshold)
                            match_result[f"{gender}å»è„‚ä½“é‡æŒ‡æ•°{operator}{threshold}"] = user_ffmi
                            
                            condition_met = HealthAssessment._evaluate_numeric_condition(user_ffmi, operator, threshold)
                            if not condition_met:
                                all_conditions_met = False
                                match_result[f"{gender}å»è„‚ä½“é‡æŒ‡æ•°{operator}{threshold}_result"] = "ä¸æ»¡è¶³"
                            else:
                                match_result[f"{gender}å»è„‚ä½“é‡æŒ‡æ•°{operator}{threshold}_result"] = "æ»¡è¶³"
                        elif not user_ffmi:
                            all_conditions_met = False
                            match_result[f"å»è„‚ä½“é‡æŒ‡æ•°{operator}{threshold}"] = "ç”¨æˆ·æ— å»è„‚ä½“é‡æŒ‡æ•°æ•°æ®"
                    elif len(match) == 2:  # ä¸åŒ…å«æ€§åˆ«çš„åŒ¹é…
                        operator, threshold = match
                        if user_ffmi is not None:
                            threshold = float(threshold)
                            match_result[f"å»è„‚ä½“é‡æŒ‡æ•°{operator}{threshold}"] = user_ffmi
                            
                            condition_met = HealthAssessment._evaluate_numeric_condition(user_ffmi, operator, threshold)
                            if not condition_met:
                                all_conditions_met = False
                                match_result[f"å»è„‚ä½“é‡æŒ‡æ•°{operator}{threshold}_result"] = "ä¸æ»¡è¶³"
                            else:
                                match_result[f"å»è„‚ä½“é‡æŒ‡æ•°{operator}{threshold}_result"] = "æ»¡è¶³"
                        else:
                            all_conditions_met = False
                            match_result[f"å»è„‚ä½“é‡æŒ‡æ•°{operator}{threshold}"] = "ç”¨æˆ·æ— å»è„‚ä½“é‡æŒ‡æ•°æ•°æ®"
            
            # æ£€æŸ¥ä½“æ€æŒ‡æ ‡æ¡ä»¶
            posture_patterns = [
                (r"é«˜ä½è‚©\s*([<>â‰¤â‰¥])\s*([\d.]+)", user_shoulder_imbalance, "é«˜ä½è‚©"),
                (r"å¤´å‰å€¾\s*([<>â‰¤â‰¥])\s*([\d.]+)", user_head_forward, "å¤´å‰å€¾"),
                (r"å¤´å€¾æ–œ\s*([<>â‰¤â‰¥])\s*([\d.]+)", user_head_tilt, "å¤´å€¾æ–œ"),
                (r"å·¦åœ†è‚©\s*([<>â‰¤â‰¥])\s*([\d.]+)", user_left_round_shoulder, "å·¦åœ†è‚©"),
                (r"å³åœ†è‚©\s*([<>â‰¤â‰¥])\s*([\d.]+)", user_right_round_shoulder, "å³åœ†è‚©"),
                (r"å·¦è…¿Xå‹\s*([<>â‰¤â‰¥])\s*([\d.]+)", user_left_leg_xo, "å·¦è…¿Xå‹"),
                (r"å³è…¿Xå‹\s*([<>â‰¤â‰¥])\s*([\d.]+)", user_right_leg_xo, "å³è…¿Xå‹")
            ]
            
            for pattern, user_value, metric_name in posture_patterns:
                matches = re.findall(pattern, condition)
                for operator, threshold in matches:
                    if user_value is not None and user_value != 0:
                        threshold = float(threshold)
                        match_result[f"{metric_name}{operator}{threshold}"] = user_value
                        
                        condition_met = HealthAssessment._evaluate_numeric_condition(user_value, operator, threshold)
                        if not condition_met:
                            all_conditions_met = False
                            match_result[f"{metric_name}{operator}{threshold}_result"] = "ä¸æ»¡è¶³"
                        else:
                            match_result[f"{metric_name}{operator}{threshold}_result"] = "æ»¡è¶³"
                    else:
                        # å¯¹äºä½“æ€æŒ‡æ ‡ï¼Œå¦‚æœç”¨æˆ·å€¼ä¸º0æˆ–Noneï¼Œå¯èƒ½è¡¨ç¤ºæ­£å¸¸
                        if threshold == 0:  # å¦‚æœé˜ˆå€¼æ˜¯0ï¼Œåˆ™ç”¨æˆ·å€¼0è¡¨ç¤ºæ»¡è¶³æ¡ä»¶
                            match_result[f"{metric_name}{operator}{threshold}"] = user_value or 0
                            match_result[f"{metric_name}{operator}{threshold}_result"] = "æ»¡è¶³"
                        else:
                            all_conditions_met = False
                            match_result[f"{metric_name}{operator}{threshold}"] = f"ç”¨æˆ·æ— {metric_name}æ•°æ®"
            
            # æ£€æŸ¥å…³é”®è¯åŒ¹é…ï¼ˆé’ˆå¯¹æ²¡æœ‰å…·ä½“æ•°å€¼æ¡ä»¶çš„è§„åˆ™ï¼‰
            if not match_result:  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ•°å€¼æ¡ä»¶ï¼Œå°è¯•å…³é”®è¯åŒ¹é…
                condition_lower = condition.lower()
                
                # ä½“æˆåˆ†å…³é”®è¯åŒ¹é…
                if any(word in condition_lower for word in ["è‚¥èƒ–", "è¿‡é‡"]) and user_bmi and user_bmi >= 25:
                    match_result["è‚¥èƒ–"] = f"BMI={user_bmi}"
                    all_conditions_met = True
                elif any(word in condition_lower for word in ["æ¶ˆç˜¦", "åç˜¦"]) and user_bmi and user_bmi < 18.5:
                    match_result["æ¶ˆç˜¦"] = f"BMI={user_bmi}"
                    all_conditions_met = True
                
                # ä½“æ€å…³é”®è¯åŒ¹é…
                posture_keyword_checks = [
                    ("é«˜ä½è‚©", user_shoulder_imbalance, 2.0),
                    ("å¤´å‰å€¾", user_head_forward, 1.0),
                    ("åœ†è‚©", max(user_left_round_shoulder or 0, user_right_round_shoulder or 0), 10.0)
                ]
                
                for keyword, value, threshold in posture_keyword_checks:
                    if keyword in condition_lower and value and value > threshold:
                        match_result[keyword] = value
                        all_conditions_met = True
            
            return all_conditions_met and len(match_result) > 0, match_result
            
        except Exception as e:
            print(f"æ£€æŸ¥è§„åˆ™é€‚ç”¨æ€§å¤±è´¥: {e}")
            return False, {"error": str(e)}
    
    @staticmethod
    def _evaluate_numeric_condition(user_value, operator, threshold):
        """è¯„ä¼°æ•°å€¼æ¡ä»¶æ˜¯å¦æ»¡è¶³"""
        if operator in ["<", "ï¼œ"]:
            return user_value < threshold
        elif operator in [">", "ï¼"]:
            return user_value > threshold
        elif operator in ["â‰¤", "<=", "â‰¦"]:
            return user_value <= threshold
        elif operator in ["â‰¥", ">=", "â‰§"]:
            return user_value >= threshold
        return False 